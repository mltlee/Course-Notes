\documentclass[10pt]{article}
\usepackage[T1]{fontenc}
\usepackage{amsmath,amssymb,amsthm}
\usepackage{mathtools}
\usepackage[shortlabels]{enumitem}
\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage{fancyhdr}
\usepackage{bold-extra}
\usepackage{color}   
\usepackage{tocloft}
\usepackage{graphicx}
\usepackage{lipsum}
\usepackage{wrapfig}
\usepackage{cutwin}
\usepackage{hyperref}
\usepackage{lastpage}
\usepackage{multicol}
\usepackage{tikz}
\usepackage{xcolor}
\usepackage{microtype}
\usepackage{titlesec}

\numberwithin{equation}{section}

% some useful math commands
\newcommand{\eps}{\varepsilon}
\newcommand{\R}{\mathbb{R}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\K}{\mathbb{K}}
\newcommand{\F}{\mathbb{F}}

\newcommand{\dd}{\,\mathrm{d}}
\newcommand{\ddz}{\frac{\rm d}{{\rm d}z}}
\newcommand{\pv}{\text{p.v.}}

\DeclareMathOperator{\GL}{GL}
\DeclareMathOperator{\id}{id}
\DeclareMathOperator{\Hom}{Hom}
\DeclareMathOperator{\End}{End}
\DeclareMathOperator{\im}{im}
\DeclareMathOperator{\Ann}{Ann}
\DeclareMathOperator{\op}{op}
\DeclareMathOperator{\Spec}{Spec}
\DeclareMathOperator{\Tr}{Tr}
\DeclareMathOperator{\ch}{char}

\newcommand{\suchthat}{\;\ifnum\currentgrouptype=16 \;\middle|\;\else\mid\fi\;}

% title formatting
\newcommand{\newtitle}[4]{
  \begin{center}
	\huge{\textbf{\textsc{#1 Course Notes}}}
    
	\large{\sc #2}
    
	{\sc #3 \textbullet\, #4 \textbullet\, University of Waterloo}
	\normalsize\vspace{1cm}\hrule
  \end{center}
}

% for theorems
\newtheoremstyle{newstyle}      
{} %Aboveskip 
{-0.25pt} %Below skip
{\mdseries} %Body font e.g.\mdseries,\bfseries,\scshape,\itshape
{} %Indent
{\scshape} %Head font e.g.\bfseries,\scshape,\itshape
{.} %Punctuation afer theorem header
{ } %Space after theorem header
{} %Heading

\theoremstyle{newstyle}

\newtheorem*{prop*}{Proposition}
\newtheorem*{cor*}{Corollary}
\newtheorem*{exercise*}{Exercise}
\newtheorem*{lemma*}{Lemma}
\newtheorem*{remark*}{Remark}
\newtheorem*{exmp*}{Example}
\newtheorem*{defn*}{Definition}
\newtheorem*{thm*}{Theorem}
\newtheorem*{notation*}{Notation}
\newtheorem{thm}{Theorem}[section]
\newtheorem{fact}[thm]{Fact}
\newtheorem{cor}[thm]{Corollary}
\newtheorem{lemma}[thm]{Lemma}
\newtheorem{remark}[thm]{Remark}
\newtheorem{prop}[thm]{Proposition}
\newtheorem{defn}[thm]{Definition}
\newtheorem{claim}[thm]{Claim}
\newtheorem{axiom}[thm]{Axiom}
\newtheorem{notation}[thm]{Notation}
\newtheorem{exercise}[thm]{Exercise}
\newtheorem{exmp}[thm]{Example}

% new proof environment
\makeatletter
\newenvironment{pf}[1][\proofname]{\par
  \pushQED{\qed}%
  \normalfont \topsep0\p@\relax
  \trivlist
  \item[\hskip\labelsep\scshape
  #1\@addpunct{.}]\ignorespaces
}{%
  \popQED\endtrivlist\@endpefalse
}
\makeatother

% 1-inch margins
\topmargin 0pt
\advance \topmargin by -\headheight
\advance \topmargin by -\headsep
\textheight 8.9in
\oddsidemargin 0pt
\evensidemargin \oddsidemargin
\marginparwidth 0.5in
\textwidth 6.5in

\parindent 0in
\parskip 1.5ex

\setlist[itemize]{topsep=0pt}
\setlist[enumerate]{topsep=0pt}

% hyperlinks
\hypersetup{
  colorlinks=true, 
  linktoc=all,     % table of contents is clickable  
  allcolors=black  % all hyperlink colours
}

% table of contents
\addto\captionsenglish{
  \renewcommand{\contentsname}%
    {Table of Contents}%
}
\renewcommand{\cftsecfont}{\normalfont}
\renewcommand{\cftsecpagefont}{\normalfont}
\cftsetindents{section}{0em}{2em}

\fancypagestyle{plain}{%
\fancyhf{} % clear all header and footer fields
\lhead{PMATH 445: Fall 2021}
\fancyhead[R]{Table of Contents}
%\headrule
\fancyfoot[R]{{\small Page \thepage\ of \pageref*{LastPage}}}
}

% headers and footers
\pagestyle{fancy}
\renewcommand{\sectionmark}[1]{\markboth{#1}{#1}}
\lhead{PMATH 445: Fall 2021}
\cfoot{}
\setlength\headheight{14pt}

%\setcounter{section}{-1}

\begin{document}

\pagestyle{fancy}
\newtitle{PMATH 445}{Representations of Finite Groups}{Jason Bell}{Fall 2021}
\rhead{Table of Contents}
\rfoot{{\small Page \thepage\ of \pageref*{LastPage}}}

\tableofcontents
\vspace{1cm}\hrule
\fancyhead[R]{\nouppercase\rightmark}
\newpage 
\fancyhead[R]{Lecture \thesection: \nouppercase\rightmark}

\section{September 8, 2021}

We begin the course by recalling Cayley's theorem, a famous result from group theory. It states that 
every finite group $G$ embeds (there exists an injective homomorphism) into a symmetric group $S_n$. 
The proof is simple: let $G$ act on itself by left multiplication, and show that this gives an 
embedding of $G$ into $S_n$ where $n = |G|$. This result is simple, but it allows us to understand 
finite groups as subgroups of symmetric groups, where one has many tools to use. 

In representation theory, one seeks to understand groups in terms of maps into general linear groups 
$\GL_n(F)$, where $F$ is a field. This is generally more desirable than an embedding into 
a symmetric group, as we obtain the full power of linear algebra at our disposal. We will consider 
all homomorphisms (not just injective ones) from groups to general linear groups, and such 
homomorphisms are called {\bf representations} of our group. First, we show that 
every finite field embeds into $\GL_n(F)$ for some field $F$.

\begin{prop}
Let $F$ be a field. Every finite group embeds into $\GL_n(F)$ for some $n \geq 1$. 
\end{prop}
\begin{pf}
Let $G$ be a finite group. By Cayley's theorem, we have an embedding $G \hookrightarrow S_n$ 
where $n = |G|$. Hence, it suffices to show that $S_n$ embeds into $\GL_n(F)$. Define 
$\varphi : S_n \to \GL_n(F)$ by $\psi(\sigma) = P_\sigma$, where $P_\sigma$ denotes the permutation matrix. Notice that for $\sigma_1, \sigma_2 \in S_n$, we have $\varphi(\sigma_1\sigma_2) 
= P_{\sigma_1\sigma_2} = P_{\sigma_1}P_{\sigma_2} = \varphi(\sigma_1\sigma_2)$, so $\varphi$ is a 
group homomorphism. One can also check that if $\varphi(\sigma) = I$, the identity matrix, then 
$\sigma$ must be the identity permutation, so $\varphi$ is injective.
\end{pf}

It turns out that this result is not true for infinite groups in general. 

\begin{exmp}
Let $G$ be the group consisting of bijective maps from $\Z^+$ to itself such that $f$ fixes all
but finitely integers. We claim that there does not exist a field $F$ and $n \geq 1$ such that 
$G$ embeds into $\GL_n(F)$. 

\begin{pf} 
First, we note the following fact from linear algebra.

{\sc Fact 1.} If $A$ and $B$ are commuting diagonalizable matrices, then they are simultaneously 
diagonalizable. That is, there is a common change of basis that makes both matrices diagonalizable.
This result also extends to families of commuting diagonalizable matrices. 

Now, we denote by $(i, j)$ the bijective mapping from $\Z^+$ to itself which swaps $i$ and $j$ 
and fixes all other integers. Consider the permutations $(1, 2)$, $(3, 4)$, $(5, 6)$, and so on. 
Note that they pairwise commute. Suppose that there exists an injective homomorphism 
$\varphi: G \to \GL_n(F)$ for some $n \geq 1$ and a field $F$. Let $A_1 = \varphi(1, 2)$, 
$A_2 = \varphi(3, 4)$, and so on. Observe that we have 
\[ \varphi((i, i+1)^2) = \varphi(\id) = I, \]
which implies that $A_1^2 = A_2^2 = \cdots = I$. We now recall another fact from linear algebra. 

{\sc Fact 2.} If the minimal polynomial of a matrix has distinct roots over the (algebraically closed) field $F$, then the matrix is diagonalizable. 

We see from above that the minimal polynomial of the $A_i$ must divide $x^2 - 1$, since 
$A_i^2 - I = 0$. As $x^2 - 1$ has distinct roots, it follows from Fact 2 that all the $A_i$ are diagonalizable. Moreover, by Fact 1, we can assume after a change of basis that each $A_i$ is of the form 
\[ A_i = \begin{pmatrix} \eps_{1,i} & & 0 \\ & \ddots & \\ 0 & & \eps_{n,i} \end{pmatrix} \]
where $\eps_{1,i}, \dots, \eps_{n,i} \in \{\pm 1\}$. Now we have a problem: there are only 
$2^n$ such matrices of the above form, and infinitely many positive integers. Thus, there 
exist positive integers $i < j$ such that $\varphi(A_i) = \varphi(A_j)$, so 
$\varphi$ is not injective, and this yields our contradiction.

Note that this argument needs an adjustment for an algebraically closed field of characteristic $2$, 
since $x^2 - 1 = (x - 1)^2$ does not have distinct roots. 
In such a case, we can proceed in the same way, except we use distinct $3$-cycles instead of $2$-cycles.
\end{pf}
\end{exmp}

We now turn to the notion of a group algebra.

\begin{defn}
The {\bf group algebra} of the group $G$ over the field $k$ is defined by 
\[ k[G] = \left\{ \sum_{g\in G} \alpha_g \cdot g : \alpha_g \in k,\, \alpha_g = 0 \text{ for all but 
finitely many $g$} \right\}. \]
\end{defn}

We note that $k[G]$ is a ring with a natural addition, and multiplication given by 
\[ \left( \sum_{g\in G} \alpha_g \cdot g \right) \left( \sum_{h \in G} \beta_h \cdot h \right) 
= \sum_{y \in G} \left( \sum_{(g, h) : gh = y} \alpha_j \cdot \beta_h \right) \cdot y. \]
Notice that the inner sum is finite because by definition, there are only finitely many 
non-zero $\alpha_g$ and $\beta_h$. 

\begin{remark}
We call $k[G]$ a group {\it algebra} because we have a ``copy'' of $k$ in $k[G]$ given by 
$\lambda \mapsto \lambda \cdot 1_G$ for elements $\lambda \in k$, with $\lambda \cdot g = g 
\cdot \lambda$ for all $g \in G$. We see that $k[G]$ is a $k$-vector space of dimension $|G|$. 
\end{remark}

\begin{exercise}
Show that $\C[S_3] \cong \C \times \C \times M_2(\C)$. Fun fact: using the naive approach 
to multiply matrices takes $O(n^3)$ operations, but applying this fact reduces the time complexity to 
$O(n^{2.373})$. 
\end{exercise}

We now prove a version of Cayley's theorem for group algebras. 

\begin{prop}
Let $G$ be a finite group with $n = |G|$. Then $G$ embeds into $\GL_n(k[G])$. 
\end{prop}
\begin{pf}
Let $G$ act on $k[G]$ by left multiplication. That is, for $g \in G$, define 
\[ L_g : k[G] \to k[G] : \sum_{h\in G} \alpha_h \cdot h \mapsto \sum_{h\in G} \alpha_h \cdot (g \cdot h). \]
Observe that for $g_1, g_2 \in G$, we have 
\[ L_{g_1} \circ L_{g_2}(x) = L_{g_1}(L_{g_2}(x)) = L_{g_1}(g_2 \cdot x) = g_1 \cdot (g_2 \cdot x)
= (g_1 \cdot g_2) \cdot x = L_{g_1g_2}(x). \] 
For the second equality, we can think of $G$ as sitting inside $k[G]$ by identifying 
$g \in G$ with $1 \cdot g \in k[G]$, so we simply have multiplication in the group algebra. 
Hence, we see that the map $L : G \to \GL_n(k[G]) : g \mapsto L_g$ is a group homomorphism. Finally, if 
$L_g$ is the identity matrix, then $g \cdot x = x$ for all $x \in k[G]$. This implies that 
$g \cdot 1 = 1$ and so $g = 1$, so $\ker L = \{1\}$. Thus, $L$ is injective and is the desired embedding.
\end{pf}

Later in the course, we will prove the following important theorem. In short, it states that if 
$G$ is a finite group and $k$ is an algebraically closed field of characteristic zero, then $k[G]$ is
isomorphic to a finite direct product of matrix rings over $k$. The isomorphism and these matrix rings
will completely determine the representation of the group $G$. 

\begin{thm}
Let $G$ be a finite group and let $k$ be an algebraically closed field of characteristic zero. 
Then we have 
\[ k[G] \cong \prod_{i=1}^s M_{n_i}(k), \]
where
\begin{enumerate}[(1)]
    \item $s$ is the number of conjugacy classes of $G$;
    \item $n_1^2 + n_2^2 + \cdots + n_s^2 = |G|$;
    \item $|\{i : n_i = 1\}| = |G/G'|$, where $G'$ denotes the commutator subgroup; and 
    \item $n_i \mid |G|$ for all $1 \leq i \leq s$. 
\end{enumerate}
\end{thm}

As a corollary of this theorem, one can prove Exercise 1.5 by noting that $\C[S_3]$ has three 
conjugacy classes: $\{(1)\}$, $\{(12), (13), (23)\}$, and $\{(123), (132)\}$. 

To finish off the lecture, we give one more interesting linear algebra fact. 

{\sc Fact.} If $q = p^j$ where $p$ is prime and $j \geq 1$, then 
\[ \lvert\GL_n(\F_q)\rvert = \prod_{i=0}^{n-1} (q^n - q^i). \]
It is not hard to see why. Let $A$ be an invertible $n \times n$ matrix, and note that $A$ must 
have linearly independent columns. For the first column, say $v_1$, we have 
$q^n - 1$ choices as we can pick any vector except the zero vector. For the second column, 
we can choose any vector except those in the span of $v_1$, which yields $q^n - q$ choices. 
One can repeat this argument to obtain the result. 

\section{September 10, 2021}

Recall that for a ring $R$, the {\bf center} of $R$ is the set 
\[ Z(R) = \{z \in R : zr = rz \text{ for all } r \in R\}. \]
Note that $Z(R)$ is a commutative subring of $R$. We can also write $[z, r] = 0$ to 
denote that $zr = rz$. 

\begin{defn}
Let $k$ be a field. We say that $R$ is a {\bf $k$-algebra} if 
\begin{enumerate}[(a)]
    \item $R$ is a ring; and 
    \item there exists a homomorphism $\phi : k \to Z(R)$ sending $1_k$ to $1_R$ (we assume that $\phi$ is injective).
\end{enumerate}
\end{defn}

Notice that if we identify $k$ with $\phi(k) \subseteq R$, we have a ``copy'' of $k$ in $R$. This 
means that $R$ is a $k$-vector space in addition to being a ring. 

\begin{defn}
Let $k$ be a field, and let $R$ and $S$ be $k$-algebras.
A {\bf $k$-algebra homomorphism} is a ring homomorphism $\psi : R \to S$ such that $\psi(\lambda) = \lambda$ for all 
$\lambda \in k$. 
\end{defn}

We also have the notion of a module. A module over a ring is a generalization of a 
vector space over a field. 

\begin{defn}
A {\bf (left) $R$-module $M$} is an abelian group $(M, +)$ equipped with a map 
\[ \cdot : R \times M \to M \] 
such that for all $r, s \in R$ and $m, n \in M$, we have 
\begin{enumerate}[(a)]
    \item $(r + s) \cdot m = r \cdot m + s \cdot m$; 
    \item $r \cdot (m + n) = r \cdot m + r \cdot n$;
    \item $(r \cdot s) \cdot m = r \cdot (s \cdot m)$; and 
    \item $1 \cdot m = m$.
\end{enumerate}
\end{defn}

\begin{exmp}
Let $R = M_n(\C)$ and $M = \C^{n \times 1}$, the set of column vectors of length $n$. One can check that 
$M$ is a (left) $R$-module equipped with the operation of matrix-vector multiplication.
\end{exmp}

\begin{exmp}
Left ideals are $R$-modules by left multiplication. In particular, $R$ itself is a left $R$-module. If 
$L$ is a left ideal of $R$ (and we write $L \trianglelefteq_\ell R$), then $L$ is a 
submodule of $R$ as a left $R$-module.
\end{exmp}

\begin{remark}
If $M_1$ and $M_2$ are $R$-modules, we define the set 
\[ \Hom_R(M_1, M_2) = \{f : M_1 \to M_2 \mid f \text{ is $R$-linear}\}. \]
That is, we have $f(r \cdot m_1 + m_2) = r \cdot f(m_1) + f(m_2)$ for all $r \in R$ and $m_1, m_2 \in M_1$. 
In the case that $M_1 = M_2 = M$, we write 
\[ \Hom_R(M_1, M_2) = \End_R(M), \]
the endomorphisms from $M$ to itself. Note that $\End_R(M)$ is a ring with composition as 
the multiplication operation, as the set of linear transformations from a vector space to itself 
forms a ring. 
\end{remark}

{\sc Fact.} If $R$ is a $k$-algebra and $M$ is a left $R$-module, then $M$ is a $k$-vector space. 

In a sense, this is clear. We know that $M$ already has scalar multiplication by $R$, and we have a 
copy of $k$ sitting inside of $R$, so if we restrict $R$ to $k$, we obtain scalar multiplication by $k$.

\begin{defn}
A left $R$-module $M$ is {\bf simple} if $M \neq (0)$, and $(0)$ and $M$ are the only submodules of $M$.
\end{defn}

\begin{exmp}
Let $R = \Z$ and $M = \Z/p\Z$ for a prime $p$. Suppose that $N \subseteq M$ is a submodule with 
$N \neq (0)$. Then there exists $i \in \{1, \dots, p-1\}$ such that the coset $[i]_p$ is in $N$. 
But this implies that $[i]_p^{-1} \cdot [i]_p = [1]_p \in N$. It follows that $N = \Z/p\Z$ and hence 
$M$ is simple. 
\end{exmp}

\begin{exercise}
Let $R = M_n(\C)$ and $M = \C^{n\times 1}$. Show that $M$ is simple. (Hint: If we are given a 
non-zero vector, then we can extend it to a basis, and we can always find a linear transformation to 
send a basis wherever we like.)
\end{exercise}

\begin{defn}
A left ideal $L$ of $R$ is a {\bf maximal left ideal} if 
\begin{enumerate}[(a)]
    \item $L \subsetneq R$; and
    \item there does not exist a left ideal $L'$ such that $L \subsetneq L' \subsetneq R$. 
\end{enumerate} 
\end{defn}

\begin{exercise}
Let $R$ be a non-zero ring (that is, $0_R \neq 1_R$). If $L$ is a proper left ideal of $R$, then 
there exists a maximal left ideal $M$ such that $L \subseteq M$. In particular, taking 
$L = (0)$ shows that a maximal left ideal always exists. 
\end{exercise}

{\sc Fact.} If $R$ is a ring and $M$ is a simple left $R$-module, then $M$ is isomorphic to 
$R/L$ (as $R$-modules), where $L$ is a maximal left ideal. 

We give a sketch of the proof of this fact. First, pick $m_0 \in M \setminus \{0\}$. Consider 
$R$ as a left $R$-module over itself (by left multiplication), and define 
\begin{align*}
    \Phi : R &\to M \\ r &\mapsto r \cdot m_0. 
\end{align*}
\begin{enumerate}[(1)]
    \item Check that $\Phi$ is an $R$-module homomorphism. 
    \item Show that $\ker\Phi = L$ is a left ideal.
    \item Show that $\im\Phi$ is a non-zero submodule of $M$, and hence must be equal to $M$ 
    (as $M$ is simple).
\end{enumerate}
By the first isomorphism theorem, it follows that $R/\ker\Phi = R/L$ is isomorphic to $M = \im\Phi$. 

Finally, why must $L$ be maximal? Similarly to the correspondence for groups and rings, we also have 
correspondence for modules. Indeed, for a module $M$ and a submodule $N \trianglelefteq M$, there 
is a bijection between the submodules of $M/N$ and the submodules of $M$ that contain $N$. 

We have the canonical projection 
\begin{align*}
    \pi : M &\to M/N \\ m &\mapsto m + N.
\end{align*} 
For a submodule $Q$ of $M/N$, notice that 
\[ \pi^{-1}(Q) = \{m \in M : \pi(m) \in Q\} \] 
is a submodule of $M$ which contains $N$, since 
\[ N = \pi^{-1}(0) \subseteq \pi^{-1}(Q). \]
Now, there is a correspondence between submodules of $M$ and the left ideals of $R$ containing $L$. 
Since $M$ is simple, it has two submodules. Thus, there are two left ideals of $R$ containing $L$, namely $L$ and $R$. Therefore, there is no ideal $L'$ such that $L \subsetneq L' \subsetneq R$, so $L$ is 
maximal.

As a corollary of the above fact, we obtain the following. 

\begin{cor}
If $R$ is a finite dimensional $k$-algebra and $M$ is a simple left $R$-module, then $M$ is a 
finite dimensional $k$-vector space.
\end{cor}

\begin{pf}
We showed that there was a surjection $\Phi : R \to M : r \mapsto r \cdot m_0$
for $m_0 \neq 0$. The fact that $\Phi$ is an $R$-module homomorphism implies that it is $k$-linear, 
as there is a copy of $k$ sitting inside of $R$. Since $R$ is finite dimensional, it follows that 
$M$ is also finite dimensional.
\end{pf}

For the remainder of this lecture, we will turn to proving Schur's lemma. 

\begin{defn}
A {\bf division ring} $\Delta$ is a ring in which every non-zero element $a \in \Delta$ 
has a multiplicative inverse $b \in \Delta$ (that is, $ab = ba = 1$).
\end{defn}

\begin{exmp}
Consider the quaternions 
\[ \mathbb{H} = \{a + ib + jc + kd : a, b, c, d \in \R\} \]
with properties 
$i^2 = j^2 = k^2 = 1$, $ij = k$, and $ji = -k$. Check that $\mathbb{H}$ is a division ring, 
but not a field.
\end{exmp}

\begin{thm}[Schur's lemma]
Let $R$ be a ring and let $M$ be a simple left $R$-module. Then $\End_R(M)$ is a division ring.
\end{thm}
\begin{pf}
We already know that $\End_R(M)$ is a ring, so we only need to show that every non-zero element 
has an inverse. Let $0 \neq f \in \End_R(M)$, which is a linear map $f : M \to M$. Notice that 
$\ker f$ is a submodule of $M$. Since $M$ is simple, $\ker f$ is either $(0)$ or $M$. The latter 
is impossible as this would mean that $f = 0$, so we have $\ker f = (0)$, so $f$ is injective. 
Similarly, $\im f$ is a submodule of $M$ and $\im f \neq (0)$ since $f \neq 0$, so 
$\im f = M$ since $M$ is simple. This shows that $f$ is surjective. Therefore, $f$ has a 
set theoretic inverse $g$. We now show that $g \in \End_R(M)$; that is, 
\[ g(r \cdot m_1 + m_2) = r \cdot g(m_1) + g(m_2) \]
for all $r \in R$ and $m_1, m_2 \in M$. But $f$ is bijective, so this is equivalent to showing that 
\[ f(g(r \cdot m_1 + m_2)) = f(r \cdot g(m_1) + g(m_2)). \]
This is indeed the case; we have 
\[ f(g(r \cdot m_1 + m_2)) = r \cdot m_1 + m_2 = r \cdot f \circ g(m_1) + f \circ g(m_2) = f(r \cdot g(m_1) + g(m_2)). \qedhere \]
\end{pf}

\section{September 13, 2021}
Recall the setting from before: we had a ring $R$, a simple left $R$-module $M$,
and $\Delta := \End_R(M)$, which we showed was a division ring. 

\begin{prop}~
\begin{enumerate}[(a)]
    \item If $R$ is a $k$-algebra for a field $k$, then $\Delta := \End_R(M)$ is also a $k$-algebra. 
    \item If $k$ is algebraically closed and $R$ is a finite-dimensional $k$-algebra, then $\Delta 
    \cong k$. 
\end{enumerate}
\end{prop}
\begin{pf}~
\begin{enumerate}[(a)]
    \item For each $\lambda \in k$, define the map 
    \begin{align*}
        \Phi_\lambda : M &\to M, \\ \lambda &\mapsto \lambda \cdot m. 
    \end{align*}
    By $\lambda \cdot m$, we mean that we have a copy $k \subseteq Z(R) \subseteq R$, and $M$ 
    is an $R$-module and hence a $k$-vector space. Note that 
    \begin{align*}
        \Phi_\lambda(r \cdot m_1 + m_2) &= \lambda \cdot (r \cdot m_1 + m_2) \\
        &= \lambda \cdot r \cdot m_1 + \lambda \cdot m_2 \\
        &= r \cdot \lambda \cdot m_1 + \lambda \cdot m_2 & (\text{since $k \subseteq Z(R)$}) \\
        &= r \cdot \Phi_\lambda(m_1) + \Phi_\lambda(m_2).
    \end{align*}
    Therefore, $\Phi_\lambda$ is $R$-linear and so $\Phi_\lambda \in \End_R(M)$. However, it is not 
    enough to show that each $\Phi_\lambda$ is in $\Delta = \End_R(M)$. We also require that our map 
    $\lambda \mapsto \Phi_\lambda$ is a map $k \to Z(\Delta)$; in other words, we also need 
    $\Phi_\lambda \in Z(\Delta)$. Let $\psi \in \Delta$, and observe that 
    \[ \Phi_\lambda \circ \psi(m) = \lambda \cdot \psi(m) = \psi(\lambda \cdot m) = \psi \circ 
    \Phi_\lambda(m), \]
    where the second equality is because the $R$-linearity of $\psi$ implies $k$-linearity.
    
    \item Recall that if $R$ is a finite dimensional $k$-algebra, then $M$ is a finite dimensional 
    $k$-vector space (see Corollary 2.12). Suppose that $\dim_k M =: n < \infty$. Then we have 
    \begin{align*}
        \Delta = \End_R(M) &\subseteq \End_k(M) & (\text{$R$-linearity imposes more conditions than 
        $k$-linearity}) \\
        &\cong\End_k(k^n) & (\text{$M$ is an $n$-dimensional $k$-vector space}) \\
        &\cong M_n(k) & (\text{$k$-linear maps $k^n \to k^n$ are the $n \times n$ matrices})
    \end{align*}
    and thus $\dim_k \Delta = m \leq n^2 < \infty$ for some $m \in \Z$. 
    
    We now show that $\Delta \cong k$. Indeed, pick $a \in \Delta$. Notice that $a$ commutes 
    with all elements of $k$ since $k \subseteq Z(\Delta)$, where we can identify $k$ with the set 
    $\{\Phi_\lambda : \lambda \in k\}$. Consider 
    \[ k \subseteq k(a) \subseteq \Delta, \]
    where $k(a)$ is the field formed from adjoining $a$ to $k$; it is a field because $\Delta$ 
    is a division ring and hence $a$ is invertible. Thus, $\dim_k k(a) \leq \dim_k \Delta = m < \infty$. 
    Therefore, $\{1, a, a^2, \dots, a^m\}$ is a linearly dependent set over $k$, so there 
    exist elements $c_0, c_1, \dots, c_m \in k$, not all zero, such that 
    \[ c_0 + c_1a + \cdots + c_m a^m = 0. \]
    But $k$ is algebraically closed, so $a \in k$. This implies that $\Delta \subseteq k$, and hence 
    $\Delta \cong k$. \qedhere 
\end{enumerate}
\end{pf}

\begin{exercise}
Let $\Delta$ be a division ring and let $M$ be a left $\Delta$-module. Then $M$ has a basis 
$B \subseteq M$. That is, there do not exist $\delta_1, \dots, \delta_m \in \Delta$ such that 
\[ \delta_1 b_1 + \cdots + \delta_m b_m = 0 \]
for distinct $b_1, \dots, b_m \in B$, and for all $m \in M$, we can write 
\[ m = \sum_{b \in B} \delta_b b \]
where $\delta_b = 0$ for all but finitely many $b \in B$. (Hint: Use Zorn's lemma.)
\end{exercise}

For this reason, instead of calling it a left $\Delta$-module, we can call it a 
{\bf left $\Delta$-vector space}.  

\begin{remark}
Let $R$ be a ring, let $M$ be a simple left $R$-module, and let $\Delta = \End_R(M)$. Then $M$ 
is a left $\Delta$-vector space. 
\end{remark}
\begin{pf}
Recall that $\delta \in \Delta$ is an $R$-linear map from $M$ to itself. We can consider the operation 
\begin{align*}
    \Delta \times M &\to M, \\
    (\delta, m) &\mapsto \delta \cdot m := \delta(m). 
\end{align*}
It is straightforward to see that 
\begin{itemize}
    \item $(\delta_1 + \delta_2) \cdot m = \delta_1 \cdot m + \delta_2 \cdot m$, 
    \item $\delta \cdot (m_1 + m_2) = \delta \cdot m_1 + \delta \cdot m_2$, 
    \item $\delta \cdot (\delta \cdot m) = (\delta_1 \cdot \delta_2) \cdot m$, and 
    \item $\id \cdot \,m = m$,
\end{itemize} so $M$ is a left $\Delta$-vector space. 
\end{pf}

\begin{exmp}
Let $R = M_n(\C)$ and recall that $M = \C^{n\times 1}$ is a simple left $R$-module. We have 
\[ \Delta = \C = \{\Phi_\lambda : \lambda \in \C\}, \]
where each $\Phi_\lambda(m) = \lambda \cdot m$ for each $\lambda \in \C$. One can show that 
$\Phi(Av) = A \cdot \Phi(v)$ for all $A \in M_n(\C)$ and $v \in \C^{n\times 1}$ only when 
$\Phi = \Phi_\lambda$ for some $\lambda \in \C$. 
\end{exmp}

We now state the Jacobson Density Theorem, which we will prove in the next lecture. 

\begin{thm}[Jacobson Density Theorem]
Let $R$ be a ring, let $M$ be a simple left $R$-module, and let $\Delta = \End_R(M)$. Then we have a 
ring $\End_\Delta(M)$ and a map 
\begin{align*}
    \Phi : R &\to \End_\Delta(M) \\
    r &\mapsto \Phi_r : M \to M \\
    & \hspace{1.33cm} m \mapsto r \cdot m.
\end{align*}
Moreover, we have the following properties. 
\begin{enumerate}[(1)]
    \item The map $\Phi$ is a ring homomorphism and if $R$ is a $k$-algebra, then $\Phi$ is a 
    $k$-algebra homomorphism.
    \item The kernel of $\Phi$ is the annihilator of $M$; that is, 
    \[ \ker\Phi = \{r \in R : r \cdot m = 0 \text{ for all } m \in M\}. \]
    \item {\bf Density:} If $m_1, \dots, m_n \in M$ are left linearly independent over $\Delta$ and 
    $w_1, \dots, w_n \in M$ are arbitrary elements, then there exists $r \in R$ such that 
    \[ \Phi_r(m_i) = w_i \]
    for all $1 \leq i \leq n$ (in particular, we can send finite linear combinations wherever we like).
\end{enumerate}
\end{thm}

For now, let's see why the maps $\Phi_r$ are $\Delta$-linear so that $\Phi_r \in \End_\Delta(M)$
for each $r \in R$. 
Let $\delta \in \Delta$ and $m_1, m_2 \in M$. Then we have 
\begin{align*}
    \Phi_r(\delta \cdot m_1 + m_2) &= r \cdot (\delta \cdot m_1 + m_2) \\
    &= r \cdot (\delta \cdot m_1) + r \cdot m_2 & (\text{multiplication by $R$ is linear}) \\
    &= r \cdot \delta(m_1) + r \cdot m_2 & (\text{we have $\delta \in \Delta$}) \\
    &= \delta(r \cdot m_1) + r \cdot m_2 & (\text{$\delta$ is an $R$-linear map}) \\
    &= \delta \cdot \Phi_r(m_1 ) + \Phi_r(m_2),
\end{align*}
so $\Phi_r$ is $\Delta$-linear as desired. 

\section{September 15, 2021}

We now prove the Jacobson Density Theorem, which we stated in the previous lecture. Recall that 
$R$ is a ring, $M$ is a simple left $R$-module, and $\Delta = \End_R(M)$. We already showed that 
$\End_\Delta(M)$ is a ring, and we defined the map 
\begin{align*}
    \Phi : R &\to \End_\Delta(M) \\
    r &\mapsto \Phi_r : M \to M \\
    & \hspace{1.33cm} m \mapsto r \cdot m.
\end{align*}

{\sc Proof of the Jacobson Density Theorem.}
\begin{enumerate}[(1)]
    \item First, observe that for all $r_1, r_2 \in R$ and $m \in M$, we have
    \begin{align*}
        \Phi(r_1r_2)(m) &= \Phi_{r_1r_2}(m) \\
        &= (r_1 \cdot r_2) \cdot m \\
        &= r_1 \cdot (r_2 \cdot m) \\
        &= r_1 \cdot \Phi_{r_2}(m) \\
        &= \Phi_{r_1} \circ \Phi_{r_2}(m) \\
        &= \Phi(r_1) \circ \Phi(r_2)(m).
    \end{align*}
    Similarly, one can show that $\Phi(r_1 + r_2) = \Phi(r_1) + \Phi(r_2)$ and $\Phi(1) = \id_M$, so 
    $\Phi$ is a ring homomorphism. Moreover, if $R$ is a $k$-algebra, then we can identify 
    $k$ with the set $\{\Phi_\lambda : \lambda \in k\}$, so we have a copy of $k$ in $Z(\Delta)$. 
    
    \item Notice that 
    \begin{align*}
        r \in \ker\Phi &\iff \Phi_r : M \to M \text{ is the zero map} \\
        &\iff \Phi_r(m) = 0 \text{ for all $m \in M$} \\
        &\iff r \cdot m = 0 \text{ for all $m \in M$,}
    \end{align*}
    where the last equivalence follows since we defined $\Phi_r$ to be left multiplication by $r$.
    
    \item We will proceed by induction on $n$. For $n = 1$, linear independence just means that 
    $m_1 \neq 0$. For arbitrary $w_1 \in R$, we need to show that there exists $r \in R$ such that 
    $\Phi_r(m_1) = r \cdot m_1 = w_1$. Let 
    \[ N = \{s \cdot m_1 : s \in R\} \subseteq M, \]
    which is an $R$-submodule of $M$. Notice that $N \neq (0)$ since $m_1 \neq 0$. Since $M$ is 
    simple, we must have $N = M$. In particular, we see that $w_1 \in N$, so there exists 
    $r \in R$ such that $r \cdot m_1 = w_1$. 
    
    Assume the result holds for $1 \leq k \leq n-1$ where $n \geq 2$. Let $m_1, \dots, m_n$ be 
    linearly independent over $\Delta$, and let $w_1, \dots, w_n \in M$ be arbitrary. We wish to find 
    $r \in R$ such that $r \cdot m_i = w_i$ for all $1 \leq i \leq n$. 
    
    By the induction hypothesis, there exists $a \in R$ such that 
    \[ a \cdot m_i = w_i \]
    for all $1 \leq i \leq n-1$. However, we don't know that $a \cdot m_n = w_n$, so we will set 
    $a \cdot m_n =: w \in M$. 
    
    {\sc Claim.} There exists $r \in R$ such that 
    \[ r \cdot m_1 = \cdots = r \cdot m_{n-1} = 0 \]
    and $r \cdot m_n =: w' \neq 0$. 
    
    To complete the proof, it is enough to prove this claim. To see why, suppose we know the claim holds. 
    We know from the base case 
    that we can send $w'$ wherever we like; in particular, there exists $b \in R$ such that 
    $b \cdot w' = w_n - w$. Notice that we have 
    \[ (a + b \cdot r) \cdot m_i = \begin{cases}
    w_i & \text{if $1 \leq i \leq n-1$,} \\ w + (w_n - w) & \text{if $i = n$.} \end{cases} \]
    Thus, choosing $a + b \cdot r \in R$ does the trick.
    
    {\sc Proof of Claim.} Suppose that no such $r \in R$ exists. In particular, if 
    \[ r \cdot m_1 = \cdots = r \cdot m_{n-1} = 0, \]
    then this will force $r \cdot m_n = 0$ as well. For $a_1, \dots, a_{n-1} \in M$, we know by the induction hypothesis that there exists 
    $s \in R$ such that 
    \[ s \cdot m_i = a_i \]
    for all $1 \leq i \leq n-1$. We can define the map $\theta : M^{n-1} \to M$ by 
    \begin{align*}
        \theta(a_1, \dots, a_{n-1}) = s \cdot m_n. 
    \end{align*}  
    Is this well-defined? We know such an $s$ exists, but we aren't guaranteed that it is unique. 
    Suppose that $s_1, s_2 \in R$ are such that 
    \[ a_i = s_1 \cdot m_i = s_2 \cdot m_i \]
    for all $1 \leq i \leq n-1$. Then 
    \[ (s_1 - s_2) \cdot m_i = a_i - a_i = 0 \] 
    for all 
    $1 \leq i \leq n-1$, and by our assumption above, we obtain 
    \[ (s_1 - s_2) \cdot m_n = 0. \]
    This means that $s_1 \cdot m_n = s_2 \cdot m_n$, so even if the choice of $s$ is not unique, the 
    map $\theta$ is well-defined. We now show that $\theta$ is $R$-linear. For $b \in R$, we have 
    \begin{align*}
        \theta(b \cdot (a_1, \dots, a_n)) 
        &= \theta(b \cdot (s \cdot m_1, \dots, s \cdot m_{n-1})) \\
        &= \theta(b \cdot s \cdot (m_1, \dots, m_{n-1})) \\
        &= (b \cdot s) \cdot m_n \\
        &= b \cdot (s \cdot m_n) \\
        &= b \cdot \theta(s \cdot (m_1, \dots, m_{n-1})) \\
        &= b \cdot \theta(a_1, \dots, a_{n-1}).
    \end{align*}
    Addition follows from the module structure, and we leave it as an exercise.
    
    For $1 \leq j \leq n-1$, we define the canonical inclusion maps 
    \begin{align*}
        i_j : M &\to M^{n-1} \\
        m &\mapsto (0, \dots, 0, m, 0, \dots, 0),
    \end{align*}
    where $m$ is placed in the $j$-th coordinate. It is easy to see that the $i_j$ are $R$-linear. 
    We have 
    \[ M \xrightarrow{i_j} M^{n-1} \xrightarrow{\theta} M \]
    where $i_j$ and $\theta$ are both $R$-linear, so we have 
    \[ \delta_j := \theta \circ i_j \in \Delta = \End_R(M). \]
    Now, we obtain 
    \begin{align*}
        \delta_1 \cdot m_1 + \cdots + \delta_{n-1} \cdot m_{n-1} 
        &= \delta_1(m_1) + \cdots + \delta_{n-1}(m_{n-1}) \\
        &= \theta \circ i_1(m_1) + \cdots + \theta \circ i_{n-1}(m_{n-1}) \\
        &= \theta(i_1(m_1) + \cdots + i_{n-1}(m_{n-1})) \\
        &= \theta(m_1, \dots, m_{n-1}) \\
        &= \theta(1 \cdot m_1, \dots, 1 \cdot m_{n-1}) \\
        &= 1 \cdot m_n \\
        &= m_n.
    \end{align*}
    This implies that $m_1, \dots, m_n$ are left linearly dependent over $\Delta$, which is a contradiction.
    Therefore, the claim holds, 
    and the result follows by induction. \qed
    
\end{enumerate}

\section{September 17, 2021}

Note that if $M$ is finite-dimensional as a $\Delta$-vector space, then the map $\Phi$ from the 
Jacobson Density Theorem is surjective. To see why, take a basis $\{m_1, \dots, m_n\}$ for $M$ as a 
$\Delta$-vector space. If $f \in \End_\Delta(M)$, there exist elements $w_1, \dots, w_n \in M$ 
such that $f(m_i) = w_i$ for all $1 \leq i \leq n$. But by the Jacobson Density Theorem, there
exists $r \in R$ such that $r \cdot m_i = w_i$ for all $1 \leq i \leq n$. 
Note that a linear transformation is completely 
determined by where the basis is sent, so $f = \Phi_r$. 

Recall that by (2) in the Jacobson Density Theorem, we have 
\[ \ker \Phi = \Ann_R(M) := \{r \in R : r \cdot m = 0 \text{ for all } m \in M\}. \]

\begin{defn}
A left $R$-module $M$ is called {\bf faithful} if $\Ann_R(M) = (0)$. 
\end{defn}

From above, we see that $M$ is faithful if and only if $\Phi$ is injective. 

\begin{defn}
A ring $R$ is said to be {\bf primitive} if it has a faithful simple module.
\end{defn}

Putting our above observations together, we obtain the following result. 

\begin{cor}
If $R$ has a faithful simple left $R$-module $M$ such that $\dim_\Delta M < \infty$ where 
$\Delta = \End_R(M)$, then the map 
\[ \Phi : R \to \End_\Delta(M) \]
from the Jacobson Density Theorem is an isomorphism. 
\end{cor}
\begin{pf}
The assumption $\dim_\Delta(M) < \infty$ gives surjectivity, and since $M$ is faithful, 
$\Phi$ is also injective.
\end{pf}

\begin{cor}
If $k$ is an algebraically closed field, $R$ is a finite-dimensional $k$-algebra, and $M$ is a 
faithful simple left $R$-module, then $R \cong M_n(k)$ where $n = \dim_k(M)$. 
\end{cor}
\begin{pf}
By Proposition 3.1, we know that $\Delta = \End_R(M) \cong k$. Moreover, we showed that 
$\dim_k M = n < \infty$ in Corollary 2.12. Therefore, we see that 
\begin{align*}
    R &\cong \End_\Delta(M) & \text{(by Corollary 5.3)} \\
    &\cong \End_k(M) & \text{(since $\Delta \cong k$)} \\
    &\cong \End_k(k^n) & \text{($M$ is an $n$-dimensional $k$-vector space)} \\
    &\cong M_n(k) & \text{($k$-linear maps $k^n \to k^n$ are the $n \times n$ matrices)} 
\end{align*}
which completes the proof.
\end{pf}

\begin{defn}
Let $R$ be a ring. We say that $R$ is {\bf left Artinian} if every descending chain of left ideals of $R$
terminates. That is, for a chain of left ideals 
\[ L_1 \supseteq L_2 \supseteq L_3 \supseteq \cdots \]
of $R$, there exists $n \geq 1$ such that $L_n = L_m$ for all $m \geq n$. 
\end{defn}

\begin{exmp}~
\begin{enumerate}[(1)]
    \item We see that $\Z$ is not left Artinian since we can take the chain of ideals 
    \[ 2\Z \supsetneq 4\Z \supsetneq 8\Z \supsetneq \cdots. \]
    \item Intuitively, $M_n(\C)$ is left Artinian since it is an $n^2$-dimensional $\C$-vector space, so
    for a chain of ideals
    \[ L_1 \supsetneq L_2 \supsetneq L_3 \supsetneq \cdots, \]
    the dimension must eventually decrease, so the chain terminates.
    \item Similarly, $\Z/6000\Z$ is left Artinian as it only has finitely many subsets, so the 
    sizes of the ideals in a descending chain must decrease. 
\end{enumerate}
\end{exmp}

We can generalize our observations from the previous example. We leave the proof as an exercise.

\begin{remark}~
\begin{enumerate}[(1)]
    \item If $R$ is a finite-dimensional $k$-algebra, then $R$ is left Artinian.
    \item If $R$ is a finite ring, then $R$ is left Artinian.
\end{enumerate}
\end{remark}

\begin{defn}
Let $R$ be a ring. Let $I$ be a two-sided ideal of $R$. We say that $I$ is a {\bf nil ideal} 
if for every $x \in I$,  there exists $n = n(x) \geq 1$ such that $x^n = 0$ (that is, 
every element in $I$ is nilpotent). 
\end{defn}

\begin{exmp}~
\begin{enumerate}[(1)]
    \item Let $R$ be any ring. Then $(0)$ is a nil ideal. 
    \item Let $R = \Z/2\Z$. Then $I = 6R = \{[0]_R, [6]_R\}$ is a nil ideal since 
    $[0]_R^1 = [0]_R$ and $[6]_R^2 = [36]_R = [0]_R$. 
    \item Let $R$ be the ring of $2 \times 2$ upper triangular matrices over $\C$; that is, 
    \[ R = \left\{ \begin{pmatrix} a&b\\0&c \end{pmatrix} : a, b,c \in \C \right\} \subseteq M_2(\C). \]
    Then one can check that 
    \[ I = \left\{ \begin{pmatrix} 0&\alpha\\0&0 \end{pmatrix} : \alpha \in \C \right\} \]
    is an ideal of $R$, and that it is a nil ideal (by squaring). 
\end{enumerate}
\end{exmp}

We now state the Artin-Wedderburn Theorem and give some remarks. 

\begin{thm}[Artin-Wedderburn]
Let $R$ be a left Artinian ring. If $R$ has no non-zero nil ideals, then there exists $s \geq 1$, 
division rings $D_1, \dots, D_s$, and integers $n_1, \dots, n_s \geq 1$ such that 
\[ R \cong M_{n_1}(D_1) \times \cdots \times M_{n_s}(D_s). \]
\end{thm}

\begin{remark}~
\begin{enumerate}[(1)]
    \item If $k$ is an algebraically closed field and $R$ is a finite-dimensional $k$-algebra, then 
    \[ R \cong M_{n_1}(k) \times \cdots \times M_{n_s}(k). \]
    \item By using Exercise 3 on Assignment 1, we see that if $R$ is also finite, then 
    \[ R \cong M_{n_1}(\F_{q_1}) \times \cdots \times M_{n_s}(\F_{q_s}). \]
    This can be observed by noting that if $R$ is finite, then the division rings must also be finite. 
    \item If $R$ is also commutative, then 
    \[ R \cong F_1 \times \cdots \times F_s \]
    for fields $F_1, \dots, F_s$. This is because commutativity forces the matrix rings to be 
    $1 \times 1$. Moreover, the division rings must also be commutative, and so they are fields.
\end{enumerate}
\end{remark}

We finish the lecture by giving one last definition. 

\begin{defn}
Let $R$ be a ring. Then a proper two-sided ideal $P$ is called a {\bf prime ideal} if whenever $a, b \in R$
are such that $aRb = \{arb : r \in R\} \subseteq P$, we either have $a \in P$ or $b \in P$.
We say that $R$ is a {\bf prime ring} if $(0)$ is a prime ideal of $R$. 
\end{defn}

\begin{exmp}
Observe that $p\Z$ for a prime $p$ is a prime ideal of $\Z$. Indeed, if $a, b \in \Z$ are such that 
$a\Z b = ab\Z \subseteq p\Z$, then $p \mid ab$. This occurs if and only if $p \mid a$ or $p \mid b$, 
or equivalently, $a \in p\Z$ or $b = p\Z$. In fact, $\Z$ is a prime ring as it has $(0)$ as a prime 
ideal. 
\end{exmp}

\section{September 20, 2021}

When $R$ is commutative, notice that $R$ is prime if and only if $R$ is an integral domain. 
Indeed, observe that 
\begin{align*}
    R \text{ is prime} &\iff aRb = (0) \text{ implies } a = 0 \text{ or } b = 0 \\
    &\iff abR = (0) \text{ implies } a = 0 \text{ or } b = 0 \\
    &\iff ab \cdot 1 = 0 \text{ implies } a = 0 \text{ or } b = 0 \\
    &\iff ab = 0 \text{ implies } a = 0 \text{ or } b = 0 \\
    &\iff R \text{ is an integral domain.}
\end{align*}
Being prime can be thought of as an extension of being an integral domain in the case where $R$ 
is not a commutative ring. 

\begin{defn}
A ring $R$ is {\bf simple} if $(0)$ and $R$ are its only two-sided ideals.
\end{defn}

\begin{prop}
If $R$ is a simple ring, then $R$ is prime. 
\end{prop}
\begin{pf}
Let $R$ be simple. Suppose that $aRb = (0)$ with $a \neq 0$ and $b \neq 0$. Since $a \neq 0$, the 
two-sided ideal 
\[ RaR := \{u_1av_1 + \cdots + u_sav_s : s \geq 0,\, u_i, \dots, u_s, v_1, \dots, v_s \in R\} \]
is equal to $R$ by the simplicity of $R$. In particular, there exists $s \geq 1$ and 
$u_s, v_1, \dots, v_s \in R$ such that 
\begin{equation}
    1 = u_1av_1 + \cdots + u_sav_s. 
\end{equation}
Similarly, there exists $t \geq 1$ and $y_1, \dots, y_t, z_1, \dots, z_t \in R$ such that 
\begin{equation}
    1 = y_1bz_1 + \cdots + y_taz_t. 
\end{equation}
Multiplying equations (6.1) and (6.2) together gives 
\[ 1 \cdot 1 = (u_1av_1 + \cdots + u_sav_s)(y_1bz_1 + \cdots + y_taz_t) 
= \sum_{i=1}^s \sum_{j=1}^t u_i(av_iy_jb)z_j = 0, \]
where the last equality is because $aRb = (0)$ and thus $av_iy_jb = 0$ for any $1 \leq i \leq s$ 
and $1 \leq j \leq t$. 
\end{pf}

\begin{prop}
Let $D$ be a division ring and let $n \geq 1$. Then $M_n(D)$ is simple and hence prime. 
\end{prop}
\begin{pf}
Let $I$ be a non-zero ideal of $M_n(D)$. We want to show that $I = M_n(D)$. Since $I$ is 
non-zero, there exists a matrix 
\[ x = \begin{pmatrix} a_{11} & \cdots & a_{1n} \\ \vdots & \ddots & \vdots \\ a_{n1} & \cdots & a_{nn} \end{pmatrix} \in I \]
with each $a_{ij} \in D$ and $a_{i_0j_0} \neq 0$ for some $1 \leq i_0, j_0 \leq n$. For all 
$1 \leq i, j \leq n$, let $e_{ij}$ be the matrix where the $(i,j)$-th entry is $1$ and all other entries
are $0$. Then we find that 
\[ e_{ii_0}xe_{j_0j} = a_{i_0j_0}e_{ij} \in I. \]
Since $D$ is a division ring, we know that $a_{i_0j_0}$ has an inverse. It follows that 
\[ \begin{pmatrix} a_{i_0j_0}^{-1} & & 0 \\ & \ddots \\ 0 & & a_{i_0j_0}^{-1} \end{pmatrix} a_{i_0j_0} e_{ij} = e_{ij} \in I \]
for all $1 \leq i, j \leq n$. In particular, we obtain 
\[ e_{11} + \cdots + e_{nn} = 1 \in I, \]
so we conclude that $I = M_n(D)$ and hence $M_n(D)$ is simple. 
\end{pf}

We now prove a nice characterization of left Artinian rings. 

\begin{prop}
Let $R$ be a ring. Then $R$ is left Artinian if and only if whenever $S$ is a non-empty collection 
of left ideals of $R$, then $S$ has a minimal element with respect to inclusion. 
\end{prop}
\begin{pf}
For the forward direction, suppose that $R$ is left Artinian. Let $S$ be a non-empty collection of 
left ideals of $R$. Pick $L_1 \in S$. If $L_1$ is minimal, we are done. Otherwise, we can pick $L_2 
\in S$ such that $L_2 \subsetneq L_1$. We can continue this process, and since $R$ is left 
Artinian, this terminates at some step $L_n$ as we cannot have an infinite strictly descending chain.

For the converse, assume that for every non-empty collection $S$ of left ideals of $R$, then 
$S$ has a minimal element with respect to inclusion. Let $L_1 \supseteq L_2 \supseteq \cdots$ 
be a descending chain of left ideals of $R$. Let $S = \{L_1, L_2, \dots\}$. By assumption, there 
exists $n \geq 1$ such that $L_n$ is a minimal element of $S$. In particular, we have 
$L_n = L_m$ for all $m \geq n$, so $R$ is left Artinian.
\end{pf}

The next theorem will be a key step towards proving the Artin-Wedderburn theorem.

\begin{thm}
Let $R$ be a prime left Artinian ring. Then $R \cong M_n(D)$ for some $n \geq 1$ and division ring 
$D$. (In fact, the converse is also true.)
\end{thm}
\begin{pf}
Let $S = \{Ru : u \in R,\, u \neq 0\}$ be a collection of left ideals of $R$. Note that $S$ 
is non-empty since $R \cdot 1 = R \in S$. Then there exists a minimal element in $S$ by Proposition 
6.4, say $Rb$ for some $b \in R$. Notice that $Rb$ is a left $R$-module (since 
left ideals are left $R$-modules). 

First, we show that $M = Rb$ is simple. If $N \subsetneq M = Rb$ is a proper left ideal with 
$N \neq (0)$, then there exists $u \neq 0$ in $N$ such that 
\[ (0) \subsetneq Ru \subsetneq N \subsetneq M. \]
But we assumed that $M = Rb$ was minimal in $S$, which is a contradiction. 

Next, we show that $M = Rb$ is faithful; that is, $\Ann_R(M) = (0)$. Suppose there exists 
$a \neq 0$ in $\Ann_R(M)$. Then we have $aM = (0)$ and hence $aRb = (0)$. Since $R$ is prime,
it must be that $a = 0$ or $b = 0$. But we assumed that $a \neq 0$ and $b \neq 0$, so this is 
a contradiction. 

Now, we show that $\dim_\Delta M < \infty$ where $\Delta = \End_R(M)$. Suppose to the contrary 
that $M$ were an infinite-dimensional left $\Delta$-vector space. Then there exist elements 
$m_1, m_2, \dots \in M$ that are $\Delta$-linearly independent. By the Jacobson Density Theorem, 
for every $n \geq 1$, there exists $r_n \in R$ such that 
\[ r_nm_1 = r_nm_2 = \cdots = r_nm_{n-1} = 0 \]
and $r_nm_n \neq 0$. Define the left ideal 
\[ L_i = \{r \in R : rm_1 = rm_2 = \cdots = rm_i = 0\} \]
for all $i \geq 1$. Notice that $L_1 \supseteq L_2 \supseteq \cdots$ and $r_n \in L_{n+1} \setminus L_n$,
which implies that these are proper containments. But this is an infinite descending chain of 
left ideals, which contradicts the fact that $R$ is left Artinian. 

In the next lecture, we will finish off the proof of this theorem.
\end{pf}

\section{September 22, 2021}

\begin{defn}
Let $S$ be a ring. The {\bf opposite ring} $S^{\op}$ of $S$ is defined to be another ring with 
the same elements and addition as $S$, but the multiplication $* : S^{\op} \times S^{\op} \to S^{\op}$ is given by 
\[ s_1 * s_2 := s_2 \cdot s_1, \]
where $\cdot$ denotes the multiplication in $S$. 
\end{defn}

\begin{remark}~
\begin{enumerate}[(1)]
    \item If $S$ is commutative, then $S^{\op}$ is the same as $S$ (since $*$ is the same as $\cdot$).
    \item If $\Delta$ is a division ring, then $\Delta^{\op}$ is also a division ring. Indeed, 
    let $a \in \Delta^{\op}$ be non-zero. Then there exists $b \in \Delta$ such that 
    $a \cdot b = b \cdot a = 1_\Delta$, so we have $b * a = a * b = 1_{\Delta^{\op}}$. Hence, 
    $a$ is invertible. 
\end{enumerate}
\end{remark}

\begin{exercise}
Let $\Delta$ be a division ring. If $M$ is an $n$-dimensional left $\Delta$-vector space, then 
$\End_\Delta(M) \cong M_n(\Delta^{\op})$. Note that if $\Delta = k$ for a field $k$, then this is just saying that $\End_k(M) \cong M_n(k)$, which is a familiar fact.

Hint: Construct the map $\Psi : \End_\Delta(M) \to M_n(\Delta^{\op})$ as follows: pick a basis 
$\{m_1, \dots, m_n\}$ as a left $\Delta$-vector space. For $f \in \End_\Delta(M)$, we have 
\[ f(m_j) = \sum_{i=1}^n a_{ij} m_i \]
where $a_{ij} \in \Delta$ since $f$ is $\Delta$-linear. Define $\Psi(f) := (a_{ij})$, and 
show that $\Psi(f \circ g) = \Psi(g) \cdot \Psi(f) = \Psi(f) * \Psi(g)$. 
\end{exercise}

Last time, we were proving Theorem 6.5, which stated that if $R$ is a prime left Artinian ring, 
then $R \cong M_n(D)$ for some $n \geq 1$ and division ring $D$. We can now finish the proof. 

We can set $n := \dim_\Delta M$ as we showed that $M$ is a finite-dimensional left $\Delta$-vector space.
Then we have 
\[ R \cong \End_\Delta(M) \cong M_n(\Delta^{\op}) \]
by Exercise 7.3, and we are done since $D = \Delta^{\op}$ is a division ring. \qed

\begin{cor}
Let $k$ be an algebraically closed field, and let $R$ be a prime finite-dimensional $k$-algebra. Then 
$R \cong M_n(k)$. 
\end{cor}
\begin{pf}
Since $R$ is a finite-dimensional $k$-algebra, it is left Artinian (see Remark 5.7). Moreover,
Proposition 3.1 shows that $\Delta = \End_R(M) \cong k$ where $M$ is a simple left 
$R$-module, since $k$ is algebraically closed. 
\end{pf}

For the rest of the lecture, we will consider the connection between prime ideals and nil ideals. 

\begin{defn}
We define the {\bf spectrum} of a ring $R$ to be the set of all prime ideals of $R$,
denoted $\Spec(R)$. 
\end{defn}

\begin{exmp}
For $R = \Z$, we have $\Spec(\Z) = \{p\Z : p \text{ prime}\} \cup \{(0)\}$. 
\end{exmp}

\begin{exmp}
For $R = M_n(\Delta)$ for $\Delta$ a division ring, we have $\Spec(M_n(\Delta)) = \{(0)\}$ as we 
showed that $M_n(\Delta)$ is a simple ring in Proposition 6.3.
\end{exmp}

\begin{exmp}
For $R = \C[x, y]$, we have
\[ \Spec(\C[x, y]) = \{(0)\} \cup \{(f) : f \text{ irreducible}\} \cup \{(x-a, y-b) : a, b \in \C\}. \]
Note that $(0)$ is a prime ideal as $\C[x, y]$ is an integral domain. Every maximal ideal of 
$\C[x, y]$ is of the form $(x-a, y-b)$, and this is due to the Nullstellensatz which we will prove. 
The other prime ideals are generated by irreducible polynomials $f$. 
\end{exmp}

\begin{thm}[Nullstellensatz]
Let $\mathfrak{M}$ be a maximal ideal of $\C[x_1, \dots, x_n]$. Then there exist 
$a_1, \dots, a_n \in \C$ such that 
\[ \mathfrak M = (x_1-a_1, \dots, x_n-a_n). \]
\end{thm}
\begin{pf}
Let $F = \C[x_1, \dots, x_n]/\mathfrak M$, which is a field because $\mathfrak M$ is maximal. 
Note that $F \supseteq \C$. Moreover, we have $\dim_{\C} F \leq \aleph_0$ since 
$\C[x_1, \dots, x_n]$ has a basis $\{x_1^{i_1} \cdots x_n^{i_n} : i_1, \dots, i_n \geq 0\} 
\cong \N^n$, which is countable. 

We claim that $F$ is algebraic over $\C$. That is, if $t \in F$, then $F$ satisfies $p(t) = 0$ 
for some $p(x) \in \C[x] \setminus \{0\}$. Note that this implies $F = \C$ since $\C$ is 
algebraically closed. 

Let $t \in F \setminus \C$. Consider the set 
\[ S = \left\{ \frac1{t-\lambda} : \lambda \in \C\right\} \subseteq F. \] 
Then $S$ is linearly dependent since $\C$ is uncountable while $\dim_\C F \leq \aleph_0$. 
Thus, there exist distinct elements $\lambda_1, \dots, \lambda_n \in \C$ and $c_1, \dots, c_n 
\in \C$, not all zero, such that 
\[ \frac{c_1}{t - \lambda_1} + \cdots + \frac{c_n}{t - \lambda_n} = 0. \]
Multiplying by $\prod_{i=1}^n (t - \lambda_i)$ gives 
\[ \sum_{i=1}^n c_i \prod_{j\neq i} (t - \lambda_j) = p(t) = 0, \]
where $p(x)$ is a polynomial in $\C[x]$. Note that $p(x)$ is non-trivial since 
\[ p(\lambda_i) = c_i \prod_{j\neq i} (\lambda_i - \lambda_j) \neq 0. \]
This proves the claim, so $\C[x_1, \dots, x_n]/\mathfrak M \cong \C$. Hence, there is a homomorphism 
\[ \phi : \C[x_1, \dots, x_n] \to \C \] 
such that $\ker\phi = \mathfrak M$. Letting $a_i = \phi(x_i)$ for all $1 \leq i \leq n$, we have 
\[ \phi(x_1^{i_1} \cdots x_n^{i_n}) = a_1^{i_1} \cdots a_n^{i_n} \]
as $\phi$ is a homomorphism. Since the $x_1^{i_1} \cdots x_n^{i_n}$ form a basis for 
$\C[x_1, \dots, x_n]$, it then follows that $\phi$ is simply the evaluation at 
$(a_1, \dots, a_n)$ map. In particular, $\mathfrak M = (x_1 - a_1, \dots, x_n - a_n)$ as desired.
\end{pf}

\begin{thm}
Let $R$ be a ring. Then the intersection of all prime ideals 
\[ \bigcap_{P \in \Spec(R)} P \]
is a nil ideal of $R$. 
\end{thm}
\begin{pf}
Let $N = \bigcap_{P \in \Spec(R)} P$. Suppose that there exists some $x \in N$ which is not nilpotent,
and let 
\[ {\cal T} = \{1, x, x^2, x^3, \dots \}. \]
Notice that $0 \notin {\cal T}$ since $x$ is not nilpotent. Let 
\[ S = \{I \trianglelefteq R : I \cap {\cal T} = \varnothing\}. \]
We have $(0) \in S$, so $S$ is non-empty. We leave it as an exercise to show that $S$ has a maximal 
element $P$. 

We claim that $P$ is a prime ideal. Suppose otherwise, so that there exists $a, b \notin P$ 
such that $aRb \subseteq P$. Since $a \notin P$, we have $RaR + P \supsetneq P$, and similarly, 
$RbR + P \supsetneq P$ as $b \notin P$. As $P$ is maximal in $S$, this implies that 
$RaR + P \notin S$, so there exists $i \geq 1$ such that $x^i \in RaR + P$. 
Analogously, we have $RbR + P \notin S$, so there exists $j \geq 1$ such that $x^j \in RbR + P$. 
It follows that 
\[ x^{i+j} = x^i \cdot x^j \in (RaR + P)(RbR + P) \subseteq R(aRb)R + P \subseteq P. \]
But this is a contradiction since $P \in S$ implies that 
$P \cap {\cal T} = \varnothing$, but we have $x^{i+j} \in P \cap {\cal T}$. 
Thus, $P$ is a prime ideal, proving the claim. 

Now, we find that $x \in N \subseteq P$ since $P$ is prime and $N$ is the intersection of all 
the prime ideals. However, $x \in {\cal T}$, which again contradicts the fact that 
$P \cap {\cal T} = \varnothing$. We conclude that every $x \in N$ must be nilpotent, so 
$N$ is a nil ideal, as required.
\end{pf}

\section{September 24, 2021}
We almost have all the tools we need to prove the Artin-Wedderburn theorem. 
First, we make a remark and prove a couple of results that we need. 

\begin{remark}
If $R$ is a left Artinian ring, then so is $R/P$ where $P$ is an ideal of $R$ by correspondence. 
Moreover, if $P$ is a prime ideal, then $R/P$ is a prime ring. The converse of this holds when 
$R$ is commutative. 
\end{remark}

\begin{lemma}
Let $R$ be a left Artinian ring. 
\begin{enumerate}[(1)]
    \item Every prime ideal of $R$ is a maximal ideal. 
    \item There are only finitely many prime ideals of $R$. 
    \item Let $P_1, \dots, P_s$ be all the prime ideals of $R$. Then for all $i = 1, \dots, s$, we have 
    \[ P_i + \bigcap_{j \neq i} P_j = R. \]
\end{enumerate}
\end{lemma}
\begin{pf}~
\begin{enumerate}[(1)]
    \item Let $P$ be a prime ideal of $R$. By Remark 8.1, $R/P$ is a prime left Artinian ring 
    and hence 
    \[ R/P \cong M_n(D) \]
    for a division ring $D$ and some $n \geq 1$ by Theorem 6.5. We know that $M_n(D)$ is simple by 
    Proposition 6.3, so its only ideals are $(0)$ and $M_n(D)$. In particular, $R/P$ is also simple. 
    By correspondence, there are only two ideals of $R$ that contain $P$. We already know that 
    $P$ and $R$ are ideals that contain $P$, so they are in fact all of them. Thus, $P$ is 
    maximal. 
    
    \item Suppose towards a contradiction that we have infinitely many distinct prime ideals 
    $P_1, P_2, \dots$ of $R$. Recall that if $I$ and $J$ are ideals of $R$, we define 
    \[ IJ = \left\{\sum_{k=1}^s i_kj_k : s \geq 1,\, i_k \in I,\, j_k \in J\right\}. \]
    We have a descending chain of ideals 
    \[ P_1 \supseteq P_1P_2 \supseteq P_1P_2P_3 \supseteq \cdots \]
    and since $R$ is left Artinian, this chain must terminate. Thus, there exists $n \geq 1$ such that 
    \[ P_1 \cdots P_n = P_1 \cdots P_n P_{n+1} \subseteq P_{n+1}. \]
    Since $P_1 \neq P_{n+1}$ and $P_1$ is a maximal ideal by (1), there exists some element 
    $a_1 \in P_1 \setminus P_{n+1}$. Similarly, for all $i = 1, \dots, n$, there exists 
    $a_i \in P_i \setminus P_{n+1}$. Then, we see that 
    \[ (a_1R)(a_2R)(a_3R) \cdots (a_{n-1}R) a_n \subseteq P_1P_2P_3 \cdots P_n \subseteq P_{n+1}, \]
    so we have 
    \begin{equation}
        a_1Ra_2R \cdots a_{n-1}Ra_n \subseteq P_{n+1}.
    \end{equation} 
    Since neither $a_1$ nor $a_2$ are in $P_{n+1}$, there exists $r_1 \in R$ such that 
    $a_1r_1a_2 \notin P_{n+1}$. This is because $P_{n+1}$ is a prime ideal of $R$, so $a_1, a_2 \notin P_{n+1}$ 
    implies that $a_1 R a_2 \nsubseteq P_{n+1}$. By (8.1), we find that 
    \[ (a_1Ra_2)Ra_3R \cdots a_{n-1}Ra_n \subseteq P_{n+1}. \]
    Now, $a_1ra_2$ and $a_3$ are both not in $P_{n+1}$, so there exists $r_2 \in R$ such that 
    $a_1r_1a_2r_2a_3 \notin P_{n+1}$ since $P_{n+1}$ is a prime ideal. Continuing in this manner, 
    we have elements 
    $r_1, r_2, \dots, r_{n-1} \in R$ such that 
    \[ a_1r_1a_2r_2 \cdots a_{n-1}r_{n-1}a_n \notin P_{n+1}, \]
    which contradicts (8.1) since 
    \[ a_1r_1a_2r_2 \cdots a_{n-1}r_{n-1}a_n \in a_1Ra_2R \cdots a_{n-1}Ra_n. \]
    We conclude that $R$ must have finitely many prime ideals.
    
    \item Let $P_1, \dots, P_s$ be the prime ideals of $R$, and fix $1 \leq i \leq s$. We claim that 
    \[ I_i := P_i + \bigcap_{j\neq i} P_j = R. \]
    Notice that $P_i \subseteq I_i \subseteq R$. Since $P_i$ is a maximal ideal by part (1), we either
    have $I_i = P_i$ or $I_i = R$. Suppose towards a contradiction that $I_i = P_i$, and recall the fact
    that $I + J = I$ if and only if $J \subseteq I$. Then we have 
    \[ \bigcap_{j\neq i} P_j \subseteq P_i, \]
    which implies that 
    \[ P_1P_2 \cdots P_{i-1}P_{i+1} \cdots P_s \subseteq \bigcap_{j\neq i} P_j \subseteq P_i. \]
    Using the same argument as the proof of (2), we can show that $P_1P_2 \cdots P_{i-1}P_{i+1} 
    \cdots P_s$ cannot be contained in the prime ideal $P_i$ as each ideal $P_j$ with $j \neq i$ is 
    prime. This contradicts our assumption that $I_i = P_i$, so we must have 
    $I_i = R$. \qedhere 
\end{enumerate}
\end{pf}

\begin{thm}[Sun-tzu]
Let $R$ be a ring, and let $I_1, \dots, I_s$ be two-sided ideals of $R$ such that 
\begin{enumerate}[(i)]
    \item $\bigcap_{j=1}^s I_j = (0)$, and 
    \item for all $i = 1, \dots, s$, we have $I_i + \bigcap_{j\neq i} I_j = R$. 
\end{enumerate}
Then we have 
\[ R \cong \prod_{i=1}^s R/I_i. \]
\end{thm}
\begin{pf}
For $i = 1, \dots, s$, we have a canonical surjection 
\begin{align*} \pi_i : R &\to R/I_i \\ r &\mapsto r + I_i, \end{align*}
which is a ring homomorphism. We define 
\begin{align*} \Psi : R &\to R/I_1 \times \cdots \times R/I_s \\ r &\mapsto (\pi_1(r), \dots, \pi_s(r)). 
\end{align*}
Note that since $\pi_1, \dots, \pi_s$ are ring homomorphisms, $\Psi$ is also a ring homomorphism. 
We now show that $\Psi$ is an isomorphism. Notice that 
\[ \ker\Psi = \{r \in R : \Psi(r) = 0\} = \{r \in R : \pi_1(r) = \cdots = \pi_s(r) = 0\} 
= \bigcap_{j=1}^s I_j = (0) \]
where the last equality follows from (i), so $\Psi$ is injective. Now, fix $1 \leq i \leq s$. 
By (ii), we have 
\[ I_i + \bigcap_{j\neq i} I_j = R, \]
so we can find $a_i \in I_i$ and $b_i \in \bigcap_{j\neq i} I_j$ such that $a_i + b_i = 1$. 
We can write $b_i = 1 - a_i$, so we find that 
\[ \pi_i(b_i) = \pi_i(1) - \pi_i(a_i) = (1 + I_i) - (0 + I_i) = 1 + I_i \]
since $a_i \in I_i$. 
On the other hand, when $n \neq i$, we have $b_i \in \bigcap_{j\neq i} I_j \subseteq I_n$, which 
gives $\pi_n(b_i) = 0 + I_n$. It follows that 
\begin{align*} \Psi(b_i) &= (\pi_1(b_i), \pi_2(b_i), \dots, \pi_i(b_i), \dots, \pi_s(b_i)) \\
&= (0 + I_1, 0 + I_2,\, \dots , 0 + I_{i-1}, 1 + I_i, 0 + I_{i+1},\, \dots, 0 + I_s). \end{align*}
Therefore, given $r_1, \dots, r_s \in R$, we have $r = r_1b_1 + \cdots + r_sb_s \in R$, and we see that
\[ \Psi(r) = \Psi(r_1b_1 + \cdots + r_sb_s) = \Psi(r_1 + I_1,\, \dots, r_s + I_s). \]
We conclude that $\Psi$ is surjective, so it is an isomorphism, as required. 
\end{pf}

\begin{thm}[Artin-Wedderburn]
Let $R$ be a left Artinian ring. If $R$ has no nonzero nil ideals, then there exists $s \geq 1$, 
division rings $D_1, \dots, D_s$, and integers $n_1, \dots, n_s \geq 1$ such that 
\[ R \cong \prod_{i=1}^s M_{n_i}(D_i). \]
\end{thm}
\begin{pf}
Let $R$ be a left Artinian ring with no nonzero nil ideals. We showed in Lemma 8.1 that $R$ 
has finitely many prime ideals; call them $P_1, \dots, P_s$. Moreover, observe that $\bigcap_{j=1}^s P_j$ is a nil ideal by Theorem 7.10, and since $R$ has no nonzero nil ideals, it must be that 
$\bigcap_{j=1}^s P_j = (0)$. We also showed that $P_i + \bigcap_{j\neq i} P_j = R$ for all 
$i = 1, \dots s$ in Lemma 8.2. It follows from Sun-tzu that 
\[ R \cong \prod_{i=1}^s R/P_i. \] 
We know that $R/P_i$ is a prime left Artinian ring by Remark 8.1, so Theorem 6.5 implies that 
$R/P_i \cong M_{n_i}(D_i)$ for some $n_i \geq 1$ and division ring $D_i$. Thus, we conclude that 
\[ R \cong \prod_{i=1}^s R/P_i \cong \prod_{i=1}^s M_{n_i}(D_i). \qedhere \]
\end{pf}

\section{September 27, 2021}

Let's first look at a corollary of Sun-tzu (Theorem 8.3).

\begin{remark}
Let $k$ be a field. If $R$ is a $k$-algebra, then each canonical surjection $\pi_i : R \to R/I_i$ 
in the proof of Sun-tzu is a $k$-algebra homomorphism, which means that the map
$\Psi : R \to R/I_1 \times \cdots \times R/I_s$ is also a $k$-algebra homomorphism. 
\end{remark}

\begin{cor}
Let $k$ be an algebraically closed field, and let $R$ be a finite-dimensional $k$-algebra with no nonzero
nil ideals. Then we have 
\[ R \cong \prod_{i=1}^s M_{n_i}(k) \]
for some $s \geq 1$ and integers $n_1, \dots, n_s \geq 1$. 
\end{cor}
\begin{pf}
Since $R$ is a finite-dimensional $k$-algebra, we see that $R$ is left Artinian (Remark 5.7).
Hence, $R$ has only finitely many prime ideals (Lemma 8.2), say $P_1, \dots, P_s$. Moreover, 
since $R$ has no nonzero nil ideals, we have $\bigcap_{j=1}^s P_j = (0)$ (Theorem 7.10), and 
$P_i + \bigcap_{j\neq i} P_j = R$ for all $i = 1, \dots, s$ since $R$ is left Artinian (Lemma 8.2). 
By Sun-tzu, we have 
\[ R \cong R/P_1 \times \cdots \times R/P_s. \]
Then, since $k$ is algebraically closed and each $R/P_i$ is a prime finite-dimensional $k$-algebra, 
Corollary 7.4 implies that 
\[ R \cong R/P_1 \times \cdots \times R/P_s \cong M_{n_1}(k) \times \cdots \times M_{n_s}(k). 
\qedhere \]
\end{pf}

Now that we have the Artin-Wedderburn theorem in our toolkit, we can finally get started with 
some representation theory. Let $G$ be a finite group with $|G| = n$, and let $k$ 
be an algebraically closed field. Recall that $V := k[G]$ is the group algebra of the group $G$ over 
the field $k$, with $\dim_k k[G] = |G| = n$. Moreover, we have the embedding 
\[ \Phi : k[G] \to \End_k(V) \cong M_n(k), \]
where we send each $g \in G$ to the left multiplication map 
\begin{align*}
    L_g : V &\to V \\ x &\mapsto g \cdot x
\end{align*}
and extend linearly over $k$. We call this the {\bf left regular representation} of $G$. 

Next, let's recall some linear algebra. Let $W$ be a vector space with basis ${\cal B} 
= \{w_1, \dots, w_n\}$, and let $T : W \to W$ be a linear map. Then we can define a matrix 
$[T]_{\cal B}$ by setting $[T]_{\cal B} = (c_{ij})$ where 
\[ Tw_j = \sum_{i=1}^n c_{ij} w_i. \] 
Observe that the trace of $T$ is given by 
\[ \Tr(T) = \sum_{j=1}^n c_{jj}, \]
which is the sum of the coefficients of $w_j$ in $Tw_j$ (and this is independent of the basis 
${\cal B}$). 

Now, for the map $L_g : V \to V$ with $g \in G$, what is $\Tr(L_g)$? First, note that the elements of 
$G$ form a basis over $V = k[G]$; that is, we have ${\cal B} = \{g_1, \dots, g_n\} = G$ in this case. 
Observe that 
\begin{equation} L_g(g_i) = g \cdot g_i = 1 \cdot g \cdot g_i + \sum_{h \in G \setminus \{gg_i\}} 0 \cdot h. \end{equation}
For $j = 1, \dots, n$, the coefficient of $g_j$ in $gg_j$ is $1$ if $g = 1$, and $0$ if 
$g \neq 1$. Indeed, if $g \neq 1$, then $g_j$ would be different than $gg_j$, so it would have 
coefficient $0$ in equation (9.1). This implies that 
\[ \Tr(L_g) = \sum_{j=1}^n \begin{cases} 1 & \text{if } g = 1 \\ 0 & \text{if } g \neq 1 \end{cases} 
= \begin{cases} n & \text{if } g = 1 \\ 0 & \text{if } g \neq 1. \end{cases} \]

\begin{thm}[Maschke]
Let $k$ be a field (not necessarily algebraically closed), and let $G$ be a finite group. If 
$\ch(k) \nmid |G|$, then $k[G]$ has no nonzero nil ideals. 
\end{thm}
\begin{pf}
Suppose towards a contradiction that $k[G]$ has a nonzero nil two-sided ideal $N$. Pick a nonzero 
element 
\[ x = \sum_{g \in G} \alpha_g g \in N. \]
Since $x$ is nonzero, there exists some $g_0 \in G$ such that $\alpha_{g_0} \neq 0$. As 
$N$ is an ideal of $k[G]$, we also have $x \cdot g_0^{-1} \in N$. The coefficient of $1$ in 
$x \cdot g_0^{-1}$ is $\alpha_{g_0} \neq 0$. Thus, we can assume without loss of generality that 
$x = \sum_{g \in G} \alpha_g g \in N$ has $\alpha_1 \neq 0$. 

Let $A \in M_n(k)$ be nilpotent so that $A^n = 0$. Let $v$ be an eigenvector of $A$ with eigenvalue $\lambda$. Then we have 
\[ Av = \lambda v \implies A^2v = \lambda^2v \implies \cdots \implies A^n v = \lambda^n v, \]
which implies that $\lambda = 0$ since $A^n = 0$. In particular, $\Tr(A)$ is the sum of the 
eigenvalues of $A$ (with multiplicity), and the above argument shows that all the eigenvalues of 
$A$ are $0$, so $\Tr(A) = 0$.

Now, consider the injective $k$-algebra homomorphism 
\begin{align*}
    \Phi : k[G] \to \End_k(V) \cong M_n(k) : \sum_{g \in G} \beta_g \cdot g \mapsto \sum_{g \in G} \beta_g \cdot L_g
\end{align*}
we discussed at the beginning of this lecture (the left regular representation of $G$). We had an 
element 
\[ x = \sum_{g \in G} \alpha_g g \in N \]
with $\alpha_1 \neq 0$. Note that $x$ is nilpotent since $N$ is a nil ideal; say
$x^j = 0$ for some $j \geq 1$. Then we have 
\[ \Phi(x)^j = \Phi(x^j) = \Phi(0) = 0, \]
so $\Phi(x)$ is also nilpotent. Our discussion above implies that $\Tr(\Phi(x)) = 0$. However, 
we also see that 
\[ \Tr(\Phi(x)) = \Tr\left( \sum_{g \in G} \alpha_g \cdot L_g \right)
= \sum_{g \in G} \alpha_g \cdot \Tr(L_g) = |G| \cdot \alpha_1 \neq 0 \]
since $\alpha_1 \neq 0$ and $\ch(k) \nmid |G|$. This is a contradiction, so $k[G]$ has 
no nonzero nil ideals. 
\end{pf}

\begin{remark}
Question 1 of Assignment 1 shows that the converse of Maschke's theorem also holds. Indeed, we 
proved that if $\ch(k) \mid |G|$, then $k[G]u$ is a nonzero nil ideal where $u = \sum_{g \in G} g$. 
\end{remark}

\begin{cor}
Let $k$ be an algebraically closed field, and let $G$ be a finite group. If $\ch(k) \nmid |G|$, then 
there exists $s \geq 1$ and integers $n_1, \dots, n_s \geq 1$ such that 
\[ k[G] \cong M_{n_1}(k) \times \cdots \times M_{n_s}(k). \]
\end{cor}
\begin{pf}
By Maschke's theorem, we know that $k[G]$ has no nonzero nil ideals, and it is left Artinian 
as it is a finite-dimensional $k$-algebra. Since $k$ is algebraically closed, it follows that 
\[ k[G] \cong M_{n_1}(k) \times \cdots \times M_{n_s}(k) \]
for some $s \geq 1$ and integers $n_1, \dots, n_s \geq 1$ by Corollary 9.2. 
\end{pf}

This corollary is important because this gives us a nice relationship between $k[G]$, which is 
something intrinsic to the group $G$, and the direct product of matrix rings, which is more in the 
realm of linear algebra. 

\end{document}