\section{More Single Machine Models} \label{sec:4}

\subsection{Maximum Lateness with Release Dates} \label{subsec:4.1}
We have already seen in Section~\ref{subsec:2.3} that there is a polynomial 
time algorithm to solve $(1\;\|\;L_{\max})$ instances. This was the 
Earliest Due Date (EDD) rule, which placed the jobs in increasing order of the 
due dates. We also saw that this was a special case of $(1 \mid \text{prec} 
\mid h_{\max})$, for which there was also an efficient algorithm, namely 
Lowest Cost Last (LCL). 

But what if we introduce release dates to the $(1\;\|\;L_{\max})$ problem? 
It turns out that this generalization, without preemption, is significantly 
harder than the problem where all jobs are available at time $0$. Moreover, 
the optimal schedule is not necessarily a non-delay schedule. It can be 
advantageous in this case to keep the machine idle before the release of a 
new job. 

\begin{theo}{theo:4.1}
    The problem $(1 \mid r_j \mid L_{\max})$ is strongly $\NP$-hard. 
\end{theo}
\begin{pf}
    This proof is based on the fact that {\sc $3$-Partition} (as described 
    in Example~\ref{exmp:3.14}) reduces to $(1 \mid r_j \mid L_{\max})$. 
    Suppose that we are given integers $a_1, \dots, a_{3t}, b$ such that 
    $\frac{b}{4} < a_j < \frac{b}{2}$ and $\sum_{j=1}^{3t} a_j = t \cdot b$. 
    We construct an instance of $(1 \mid r_j \mid L_{\max})$ with 
    $n = 4t - 1$ as follows: 
    \begin{itemize}
        \item For $j = 1, \dots, t-1$, we set $r_j = jb + (j-1)$, $p_j = 1$, 
        $d_j = jb + j$. 
        \item For $j = t, \dots, 4t-1$, we set $r_j = 0$, $p_j = a_{j-t+1}$, 
        and $d_j = tb + (t-1)$. 
    \end{itemize}
    Notice that a schedule with $L_{\max} \leq 0$ exists if and only if 
    every job $j \in \{1, \dots, t-1\}$ can be processed between 
    $r_j$ and $d_j = r_j + p_j$. This can be done if and only if the remaining 
    jobs can be partitioned over the $t$ intervals of length $b$, which can be 
    done if and only if {\sc $3$-Partition} has a solution. 
\end{pf}

The $(1 \mid r_j \mid L_{\max})$ problem is important because it often 
appears as a subproblem in heuristic procedures for flow shop and job shop 
problems. A branch and bound procedure $(1 \mid r_j \mid L_{\max})$ 
can be constructed as follows. The branching process may be based on the 
fact that schedules are developed starting from the beginning of the schedule. 
There is a single node at level $0$ which is the top of the tree. At this 
node, no job has been put into any position of the sequence yet. There are 
$n$ branches going down to $n$ nodes at level $1$. Each node at this level 
has a specific job put into the first position of the schedule. Then
at each node, there are $n-1$ jobs remaining whose position in the schedule 
has yet to be determined. Hence, there are $n-1$ arcs emanating from each 
node at level $1$ to level $2$, and there are $(n-1) \times (n-2)$ nodes at 
level $2$. We could continue in this way to enumerate all possible schedules. 

However, it is not necessary to consider every remaining job as a possible 
candidate for the next position. If the jobs $j_1, \dots, j_{k-1}$ 
are scheduled as the first $k-1$ jobs at a node at level $k-1$, then 
we only need to consider job $j_k$ if 
\[ r_{j_k} < \min_{\ell \in J} \left( \max(t, r_\ell) + p_\ell \right), \] 
where $J$ denotes the set of jobs not yet scheduled and $t$ denotes the 
time job $j_k$ is supposed to start. The reason for this condition is because 
if job $j_k$ does not satisfy this inequality, then selecting the job 
which minimizes the right-hand side instead of $j_k$ does not increase the 
value of $L_{\max}$. Therefore, the branching rule is fairly easy. 

There are several ways in which bounds for nodes can be obtained. 
One easy lower bound for a node at level $k-1$ can be established by 
scheduling the remaining jobs $J$ according to the \emph{preemptive} 
EDD rule which is known to be optimal for $(1 \mid r_j, \text{prmp} \mid 
L_{\max})$, and thus provides a lower bound for the problem at hand. 
If a preemptive EDD rule results in a non-preemptive schedule, then 
all nodes with a higher lower bound may be disregarded. 

\begin{exmp}{exmp:4.2}
    Consider the following instance of $(1 \mid r_j \mid L_{\max})$ with 
    $n = 4$ jobs. 
    \begin{align*}
        \begin{array}{c|cccc} 
            j & 1 & 2 & 3 & 4 \\ \hline 
            p_j & 4 & 2 & 6 & 5 \\ 
            r_j & 0 & 1 & 3 & 5 \\ 
            d_j & 8 & 12 & 11 & 10 
        \end{array}
    \end{align*}
    At level $1$ of the search tree, there are four nodes, namely 
    $(1, *, *, *)$, $(2, *, *, *)$, $(3, *, *, *)$, and $(4, *, *, *)$. 
    Notice that we may discard the nodes $(3, *, *, *)$ and 
    $(4, *, *, *)$ immediately. Indeed, we have $\min_{\ell\in \{1, 2, 3, 4\}} 
    (\max(t, r_\ell) + p_\ell) = 3$ with $\ell = 2$, and we see that 
    $r_3 \geq 3$ and $r_4 \geq 3$. 

    Computing a lower bound for node $(1, *, *, *)$ according to the 
    preemptive EDD rule results in a schedule where job $3$ is processed 
    during the interval $[4, 5]$, job $4$ is processed during $[5, 10]$, 
    job $3$ again during $[10, 15]$, and job $2$ during $[15, 17]$. 
    Then $L_{\max} = 5$ for this schedule, and so $5$ is a lower bound 
    for the node $(1, *, *, *)$. A similar computation shows that a lower 
    bound for the node $(2, *, *, *)$ is $7$. 

    Consider now the node $(1, 2, *, *)$ at level $2$. The lower bound 
    for this node is $6$ and is determined by the non-preemptive 
    schedule $1, 2, 4, 3$. Next, looking at the node $(1, 3, *, *)$ 
    at level $2$, the lower bound is $5$ and is determined by the 
    non-preemptive schedule $1, 3, 4, 2$. Since the lower bound for node 
    $(1, *, *, *)$ is $5$ and the lower bound for $(2, *, *, *)$ is 
    greater than $5$, it follows that the schedule $1, 3, 4, 2$ is optimal. 
\end{exmp}

The problem $(1 \mid r_j, \text{prec} \mid L_{\max})$ can be handled in a similar 
way. From an enumeration point of view, it is even easier than the problem 
without precedence constraints since many schedules can be ruled out 
immediately. 

\subsection{Number of Tardy Jobs} \label{subsec:4.2}
Recall that $U_j = 0$ if the job is timely and $U_j = 1$ if the job is late. 
The goal of the problem $(1\;\|\;\sum U_j)$ is to minimize the number of 
tardy jobs. This objective may at first appear somewhat artificial and 
seems to be of no practical interest. However, in the real world, it is a 
performance measure that is often monitored. It is equivalent to the 
percentage of on time shipments.

Notice that it does not matter how late a job is; the only 
determining factor is if it is late or not. A solution to this problem can 
be represented as a partition of the jobs into sets $S_1$ and $S_2$, where 
$S_1$ is the set of jobs meeting their due dates in Earliest Due Date (EDD)
order, and $S_2$ is the set of late jobs in an arbitrary order (since the 
amount of lateness is irrelevant).  

\begin{lemma}{lemma:4.3}
    Let OPT denote the optimal value for a given instance of $(1\;\|\;\sum U_j)$.
    If the sequence given by the EDD rule has a late job, then $\text{OPT} \geq 1$. 
\end{lemma}
\begin{pf}
    Let $k$ be the first late job in the EDD sequence. Then we have 
    \[ C_k = \sum_{i\in[k]} p_i > d_k = \max_{i\in[k]} d_i. \] 
    Consider any schedule $S$. Let $\ell$ be the last of the jobs in $[k]$ in 
    $S$. Then the completion time of $\ell$ in $S$ is at least $\sum_{i\in[k]} 
    p_i > d_\ell$, so $\ell$ is a late job in $S$. 
\end{pf}

\begin{algo}[Moore-Hodgson]{algo:4.4}
    \begin{enumerate}
        \item Enumerate the jobs in EDD order. 
        \item Set $S_1 \gets \varnothing$ and $t \gets 0$. 
        \item for $j=1$ to $n$ do: 
        \begin{itemize}[\label{}]
            \item Set $S_1 \gets S_1 \cup \{j\}$ and $t \gets t + p_j$. 
            \item if $t > d_j$ then: 
            \begin{itemize}[\label{}]
                \item Find a job $k$ with the largest $p_k$ value in $S_1$. 
                \item Set $S_1 \gets S_1 \setminus \{k\}$ and $t \gets t - p_k$. 
            \end{itemize}
            endif
        \end{itemize}
        endfor
    \end{enumerate}
\end{algo}

The principle of the Moore-Hodgson algorithm is that we schedule the jobs by the EDD 
rule, and when a job gets late, we rescue the situation by throwing out the job with 
the highest processing time. All removed jobs are considered late, and the remaining 
ones are timely. This algorithm runs in $O(n\log n)$ time. 

\begin{exmp}{exmp:4.5}
    Consider the following instance of $(1 \;\|\; \sum U_j)$ with $n = 5$ jobs, 
    where the jobs are already in EDD order.  
    \begin{align*}
        \begin{array}{c|ccccc}
            j   & 1 & 2  & 3  & 4  & 5 \\ \hline 
            p_j & 7 & 8  & 4  & 6  & 6 \\ 
            d_j & 9 & 17 & 18 & 19 & 21
        \end{array}
    \end{align*}
    We can initially schedule jobs $1$ and $2$, which will both be timely. However, 
    once we schedule job $3$, we see that it will be late, since $t = 19 > 18 = d_3$.
    \begin{verbatim}
        |---1---|---2----|-3--|
        0       7        15   19
    \end{verbatim}
    \vspace{-1em}
    Therefore, we toss out job $2$ which has the highest processing time of $p_2 = 8$
    and continue. We can schedule job $4$ just fine, but job $5$ will be late with 
    $t = 23 > 21 = d_5$. 
    \begin{verbatim}
        |---1---|-3--|--4---|--5---|
        0       7    11     17     23
    \end{verbatim}
    \vspace{-1em}
    This time, we toss out job $1$ since it has the highest processing time of $p_1 = 7$. 
    Then we obtain $S_1 = \{3, 4, 5\}$ and $S_2 = \{1, 2\}$, so $\text{OPT} = 2$ for 
    this instance. 
\end{exmp}

The following lemma is the key to proving that the Moore-Hodgson algorithm is 
optimal for $(1\;\|\;\sum U_j)$. We will assume that the jobs are already scheduled 
in EDD. 

\begin{lemma}{lemma:4.6}
    Suppose there is at least one late job in the EDD sequence $1, \dots, n$. 
    Let $k$ be the first late job in the EDD sequence, and let $m$ be the first 
    job rejected by the Moore-Hodgson algorithm. Then there is an optimal 
    schedule which rejects $m$. 
\end{lemma}
\begin{pf}
    Consider any optimal schedule $\pi$. Let $R_\pi \subseteq [n]$ denote 
    the subset of rejected (late) jobs, and let $A_\pi = [n] \setminus R_\pi$ 
    denote the set of timely jobs. By Lemma~\ref{lemma:4.3}, we may assume that 
    $\pi$ schedules the jobs of $A_\pi$ in EDD order, followed by the 
    jobs of $R_\pi$ in arbitrary order. 

    If $m \in R_\pi$, we are done. So suppose that $m \notin R_\pi$. 
    By Lemma~\ref{lemma:4.3}, there is a job $r \in [k]$ other than $m$ 
    that has been rejected. Consider the schedule $\sigma$ that sequences 
    the jobs in $A_\sigma = (A_\pi \setminus \{m\}) \cup \{r\}$ in EDD 
    order first, followed by the jobs in $[n] \setminus A_\sigma = 
    (R_\pi \setminus \{r\}) \cup \{m\}$ in arbitrary order. We will show that 
    $\sigma$ schedules all jobs in $A_\sigma$ on time. This will mean that 
    $\sigma$ is an optimal schedule since $|R_\sigma| = |R_\pi|$, 
    and by construction, $\sigma$ rejects $m$.

    First, the jobs in $A_\sigma \cap [k-1]$ are completed on time 
    since the EDD rule completes all jobs in $[k-1]$ on time. Next, 
    if $k \in A_\sigma$, then its completion time is at most 
    \[ \sum_{i\in[k] \setminus \{m\}} p_i \leq 
    \sum_{i\in[k-1]} p_i \leq d_{k-1} \leq d_k, \] 
    where the first inequality follows from the choice of $m$ by the 
    Moore-Hodgson algorithm which ensures that $p_m = 
    \max_{i\in[k]} p_i \geq p_k$. Finally, compared to the schedule $\pi$, 
    the completion times of the jobs in $A_\sigma \setminus [k] = 
    A_\pi \setminus [k]$ have been changed in the new schedule $\sigma$
    by $p_r - p_m \leq 0$, so these jobs are also on time. 
\end{pf}
 
\begin{theo}{theo:4.7}
    The Moore-Hodgson algorithm gives an optimal schedule for 
    $(1\;\|\;\sum U_j)$. 
\end{theo}
\begin{pf}
    We proceed by induction on $\text{OPT}$ over all instances of the problem. 

    If an instance has $\text{OPT} = 0$, then by Lemma~\ref{lemma:4.3}, the 
    Moore-Hodgson algorithm outputs the EDD sequence with no late jobs. 

    Suppose now that $I$ is an instance with $\text{OPT}(I) \geq 1$ late 
    jobs. Let $I'$ be obtained from $I$ by deleting the first job $m$ 
    which is rejected by the Moore-Hodgson algorithm. By Lemma~\ref{lemma:4.6}, 
    we have $\text{OPT}(I') = \text{OPT}(I) - 1$. By the induction hypothesis, 
    the algorithm finds an optimal schedule $S'$ for $I'$. The algorithm 
    for $I$ outputs the schedule $S$ such that $S$ is the same as $S'$ 
    except that job $m$ is added at the end as a rejected job. Clearly, 
    $S$ has at most $\text{OPT}(I') + 1 = \text{OPT}(I)$ rejected jobs 
    and hence is optimal. 
\end{pf}

\subsection{Total Tardiness} \label{subsec:4.3}
In practice, minimizing the number of tardy jobs $\sum U_j$ cannot be the 
only objective to measure how due dates are being met. Only minimizing 
the number of late jobs will force some jobs to have to wait for an 
unacceptably long time to complete. If we instead minimize the total 
tardiness, it is less likely that the wait for any given job will be 
unacceptably long. 

The problem $(1\;\|\;\sum T_j)$ has received an enormous amount of attention 
in literature. For many years, its computational complexity remained open; 
its $\NP$-hardness was only established recently in 1990. Since 
$(1\;\|\;\sum T_j)$ is $\NP$-hard in the ordinary sense, it allows for a 
pseudo-polynomial time algorithm based on dynamic programming. The 
algorithm is based on two preliminary results. 

\begin{lemma}{lemma:4.8}
    If $p_j \leq p_k$ and $d_j \leq d_k$, then there exists an optimal schedule 
    in which job $j$ is scheduled before job $k$. 
\end{lemma}
\begin{pf}
    We leave the proof as an exercise. This is a standard interchange 
    argument. 
\end{pf}

This type of result is useful when an algorithm has to be developed for
a problem that is $\NP$-hard. Such a result, often referred to as a 
{\bf dominance result} or {\bf elimination criterion}, often allows one to
disregard a fairly large number of sequences. Such a dominance result may also 
be thought of as a set of precedence constraints on the jobs. The more 
precedence constraints created through such dominance results, the easier the 
problem becomes.

In the following lemma, the sensitivity of an optimal sequence to the due
dates is considered. We consider two problem instances, both of which have 
$n$ jobs with processing times $p_1, \dots, p_n$. The first instance has 
due dates $d_1, \dots, d_n$. Let $k$ be a fixed job, and let $\hat C_k$ 
be the latest completion time of job $k$ among all optimal schedules. 
Then we set the due dates for the second instance to be 
\[ \hat d_j = \begin{cases}
    d_j, & \text{if } j \neq k, \\ 
    \max(d_k, \hat C_k), & \text{if } j = k.
\end{cases} \] 
In particular, we are only changing one piece of data, namely the 
due date of job $k$. 

\begin{lemma}{lemma:4.9}
    Any optimal sequence with respect to the second instance with due dates 
    $\hat d_1, \dots, \hat d_n$ is also optimal for the first instance 
    with due dates $d_1, \dots, d_n$. 
\end{lemma}
\begin{pf}
    We first introduce some notation. Let $\hat S$ be an optimal sequence 
    such that job $k$ has largest completion time $\hat C_k$ among 
    all optimal sequences. For an arbitrary sequence $S$, we let 
    $V(S)$ be the objective value according to the first instance 
    where job $k$ has due date $d_k$, and $V'(S)$ be the objective 
    value according to the second instance where job $k$ has due date 
    $\hat d_k$. 

    Let $S'$ be any optimal sequence for the second instance. Then we have 
    $V'(\hat S) \geq V'(S')$ since $S'$ is optimal for the second instance. 
    Moreover, we observe that 
    \[ V'(\hat S) = V(\hat S) - (\hat C_k - d_k), \] 
    or equivalently, $V(\hat S) - V'(\hat S) = \hat C_k - d_k$. Finally, 
    when we switch objectives, the most improvement we can get is 
    $\hat C_k - d_k$, so we deduce that 
    \begin{align*}
        V(S') &\leq V'(S') + (\hat C_k - d_k) \\ 
        &= V'(S') + V(\hat S) - V'(\hat S) \\ 
        &\leq V(\hat S).
    \end{align*}
    This means that $S'$ is also optimal for the first instance, so we are done. 
\end{pf}

Lemma~\ref{lemma:4.9} is quite a technical result, and it requires the 
knowledge of all optimal sequences for the first instance. 
For now, we will assume that the jobs are scheduled in EDD order 
so that $d_1 \leq \cdots \leq d_n$, and job $k$ is such that 
$p_k = \max(p_1, \dots, p_n)$. In particular, this is the job with 
the $k$-th smallest due date and largest processing time. It follows from 
Lemma~\ref{lemma:4.8} that there exists an optimal sequence in which 
jobs $1, \dots, k-1$ all appear in some order before job $k$. Of the remaining 
$n-k$ jobs, some may appear before job $k$ while some may appear after job $k$. 
The following lemma focuses on the placement of these remaining jobs. 

\begin{lemma}{lemma:4.10}
    There is an integer $\delta$ with $0 \leq \delta \leq n-k$ such that there 
    is an optimal sequence $S$ in which job $k$ is preceded by all jobs $j$ 
    with $j \leq k + \delta$ and followed by all jobs $j$ with $j > k + \delta$. 
\end{lemma}
\begin{pf}
    We only give a brief sketch here. In some optimal schedule, the jobs 
    $1, \dots, k-1$ are positioned before 
    job $k$ due to the dominance criterion from Lemma~\ref{lemma:4.8}. 
    From Lemma~\ref{lemma:4.9}, we can increase $d_k$ such that some other 
    jobs $k+1, \dots, k+\delta$ are positioned before job $k$ due to the 
    dominance criterion. 
\end{pf}

Lemma~\ref{lemma:4.10} tells us an optimal sequence looks like the concatenation 
of three subsets of jobs, namely 
\begin{enumerate}[(i)]
    \item jobs $1, \dots, k-1, k+1, \dots, k+\delta$ in some order, followed by 
    \item job $k$, followed by 
    \item jobs $k+\delta+1, \dots, n$ in some order. 
\end{enumerate}
The completion time of job $k$ under this schedule is 
\[ C_k(\delta) = \sum_{j\leq k+\delta} p_j. \] 
For the entire sequence to be optimal, it is clear that the first and third 
subsets must be optimally sequenced within themselves. This is the heart of 
a dynamic programming approach that determines an optimal sequence for a 
larger set of jobs after having determined optimal sequences for proper 
subsets of the larger set. 

We define $J(j, \ell, k)$ to be the set of all jobs in the set $\{j, \dots, \ell\}$ 
with processing time at most $p_k$, but $k \notin J(j, \ell, k)$. 
Note that here, $k$ does not necessarily have to be the job with highest processing 
time. Then, we define $V(J(j, \ell, k), t)$ to be the total tardiness of 
the jobs $J(j, \ell, k)$ in an optimal sequence that starts at time $t$. 
We now state the dynamic programming algorithm. 

\begin{algo}[Dynamic Programming for Total Tardiness]{algo:4.11}
    The initial conditions are $V(\varnothing, t) = 0$ and 
    $V(\{j\}, t) = \max(0, t + p_j - d_j)$ for all jobs $j$ and $t \geq 0$. 
    We have the recursive relation 
    \[ V(J(j, \ell, k), t) = \min_{\delta} \{V(J(j, k'+\delta, k'), t)
    + \max(0, C_{k'}(\delta) - d_{k'}) + V(J(k'+\delta+1, \ell, k'), 
    C_{k'}(\delta))\}, \] 
    where job $k'$ is such that $p_{k'} = \max_{j'\in J(j,\ell, k)} p_{j'}$. 
    The optimal value is then given by $V(\{1, \dots, n\}, 0)$. 
\end{algo}

Note that there are at most $O(n^3)$ subsets of jobs $J(j, \ell, k)$ and 
$\sum p_j$ possible values of $t$. This means that there are 
$O(n^3 \sum p_j)$ recursive equations. Each recursion takes $O(n)$ time, 
so the total running time of the dynamic programming approach is 
$O(n^4 \sum p_j)$, which is pseudo-polynomial. 

\begin{exmp}{exmp:4.12}
    We run our dynamic programming algorithm on an example. Consider 
    the following instance of $(1\;\|\;\sum T_j)$ with $n = 5$ jobs. 
    \begin{align*}
        \begin{array}{c|ccccc}
            j & 1 & 2 & 3 & 4 & 5 \\ \hline 
            p_j & 121 & 79 & 147 & 83 & 130 \\ 
            d_j & 260 & 266 & 266 & 336 & 337 
        \end{array}
    \end{align*}
    We see that job $3$ has the largest processing time, giving us 
    $0 \leq \delta \leq 5 - 3 = 2$. Then we have 
    \[ V(\{1, 2, 3, 4, 5\}, 0) = \min\begin{cases} 
        V(J(1, 3, 3), 0) + 81 + V(J(4, 5, 3), 347), & \text{for } \delta = 0, \\ 
        V(J(1, 4, 3), 0) + 164 + V(J(5, 5, 3), 430), & \text{for } \delta = 1, \\ 
        V(J(1, 5, 3), 0) + 294 + V(\varnothing, 560), & \text{for } \delta = 2. 
    \end{cases} \] 
    We see that $V(J(1, 3, 3), 0) = 0$ via the sequences $1, 2$ and $2, 1$. 
    The dominance rule tells us that the sequences $1, 2, 4$ and $2, 1, 4$ 
    are optimal for the jobs $J(1, 4, 3)$ with $V(J(1, 4, 3), 0) = 0$, 
    and similarly, the sequences $1, 2, 4, 5$ and $2, 1, 4, 5$ are optimal for 
    $J(1, 5, 3)$ with $V(J(1, 5, 3), 0) = 76$.

    On the other hand, we have 
    \[ V(J(4, 5, 3), 347) = (347 + 83 - 336) + (347 + 83 + 130 - 337) = 317 \] 
    for the sequence $4, 5$, and $V(J(5, 5, 3), 430) = 430 + 130 - 337 = 223$.

    Putting everything together, we obtain 
    \[ V(\{1, 2, 3, 4, 5\}, 0) = 
    \min\{0 + 81 + 317, 0 + 164 + 223, 76 + 294 + 0\}
    = \min\{398, 387, 370\} = 370. \] 
    The sequences that attain this value are $1, 2, 4, 5, 3$ and $2, 1, 4, 5, 3$.    
\end{exmp}

Since $(1\;\|\;\sum T_j)$ is $\NP$-hard, we cannot hope to find a polynomial 
time algorithm to solve arbitrary instances of it. However, we can give an FPTAS
to obtain a solution that is close to optimal, using a similar approach 
as we did for the {\sc Knapsack} problem in Section~\ref{subsec:3.4}. 

First, we will give some lower and upper bounds for total tardiness. 
\begin{itemize}
    \item Recall that the EDD rule produces a schedule with optimal 
    maximum lateness. In particular, this also produces a schedule 
    with optimal maximum tardiness. 
    \item Clearly, at least one job in a schedule has the maximum tardiness. 
    Then the total tardiness of an optimal schedule is at least the 
    optimal maximum tardiness. 
    \item The total tardiness of the EDD schedule is at least the optimal 
    maximum tardiness, but at most $n$ times the optimal maximum tardiness. 
\end{itemize}
Let $\sum T_j(\text{EDD})$ denote the total tardiness under the EDD schedule, 
and let $T_{\max}(\text{EDD}) = \max(T_1, \dots, T_n)$ be the maximum 
tardiness under the EDD schedule. For an optimal schedule $\text{OPT}$, we have 
\[ T_{\max}(\text{EDD}) \leq \sum T_j(\text{OPT}) \leq \sum T_j(\text{EDD}) 
\leq n \cdot T_{\max}(\text{EDD}). \] 
Let $V(J, t)$ be the minimum total tardiness of the job subset $J$ 
assuming that processing begins at time $t$. There is a time $t^*$ 
such that $V(J, t) = 0$ for all $t \leq t^*$ and $V(J, t) > 0$ for all 
$t > t^*$. Then we have $V(J, t^* + \delta) \geq \delta$ for all 
$\delta \geq 0$. By executing the pseudo-polynomial dynamic programming 
approach described in Algorithm~\ref{algo:4.11}, one only has to compute 
$V(J, t)$ for 
\[ \max\{0, t^*\} \leq t \leq t^* + n \cdot T_{\max}(\text{EDD}). \] 
Substituting $\sum p_j$ in the overall running time of the dynamic 
programming algorithm by $n \cdot T_{\max}(\text{EDD})$ yields a 
new bound of $O(n^5 \cdot T_{\max}(\text{EDD}))$. 

Now, rescale the processing times and due dates via $p'_j = 
\lfloor p_j/K \rfloor$ and $d'_j = d_j/K$ for some factor $K > 0$. 
Let $S$ be an optimal schedule for the rescaled problem. We let 
$\sum T_j^*(S)$ denote the total tardiness of the schedule $S$ with 
respect to the processing times $Kp'_j \leq p_j$ and due dates $d_j$. 
Let $\sum T_j(S)$ denote the total tardiness of the schedule $S$ with 
respect to the original processing times $p_j$ and due dates $d_j$. 
From the fact that $Kp'_j \leq p_j < K(p'_j + 1)$, it follows that 
\[ \sum T_j^*(S) \leq \sum T_j(\text{OPT}) \leq \sum T_j(S) < 
\sum T_j^*(S) + \frac{Kn(n+1)}{2}. \] 
This chain of inequalities implies that 
\[ \sum T_j(S) - \sum T_j(\text{OPT}) < \frac{Kn(n+1)}{2}. \] 
If $K$ is chosen such that 
\[ K = \frac{2\eps}{n(n+1)} \cdot T_{\max}(\text{EDD}), \] 
then we obtain 
\[ \sum T_j(S) - \sum T_j(\text{OPT}) < \eps \cdot T_{\max}(\text{EDD}). \] 
Moreover, for this choice of $K$, the time bound 
$O(n^5 \cdot T_{\max}(\text{EDD})/K)$ becomes $O(n^7/\eps)$, making 
our approximation scheme fully polynomial. We summarize the FPTAS 
as follows. 

\begin{algo}[Total Tardiness FPTAS]{algo:4.13}
    \begin{enumerate}[(1)]
        \item Apply EDD in order to determine $T_{\max}(\text{EDD})$. 
        If $T_{\max}(\text{EDD}) = 0$, then $\sum T_j(\text{OPT}) = 0$ 
        and EDD gives an optimal schedule, so stop. Otherwise, set 
        \[ K = \frac{2\eps}{n(n+1)} \cdot T_{\max}(\text{EDD}). \] 
        \item Rescale the processing times and due dates via 
        $p'_j = \lfloor p_j/K \rfloor$ and $d'_j = d_j/K$. 
        \item Apply Algorithm~\ref{algo:4.11} to the rescaled data. 
    \end{enumerate}
\end{algo}

\subsection{Weighted Number of Late Jobs} \label{subsec:4.4}
The generalization of the problem $(1\;\|\;\sum U_j)$ with additional weights 
is known to be $\NP$-hard. This can be shown via a reduction from 
{\sc $3$-Partition}. A popular heuristic for this problem is the WSPT rule, 
but even that can perform very badly. As usual, we cannot hope to find a 
polynomial time algorithm which solves $(1\;\|\;\sum w_j U_j)$. 

\begin{exmp}{exmp:4.14}
    Consider the following instance of $(1 \mid d_j = d \mid \sum w_j U_j)$ with 
    $n = 3$ jobs and equal due dates. 
    \begin{align*}
        \begin{array}{c|ccc}
            j & 1 & 2 & 3 \\ \hline 
            p_j & 11 & 9 & 90 \\ 
            d_j & 100 & 100 & 100 \\ 
            w_j & 12 & 9 & 89
        \end{array}
    \end{align*}
    Then the WSPT rule fails: it gives the schedule $1, 2, 3$ which has 
    objective value $89$. The optimal schedules for this example are 
    $2, 3, 1$ and $3, 2, 1$, which both have objective value $12$. 
\end{exmp}

In fact, the {\sc Knapsack} problem is equivalent to the special case 
$(1 \mid d_j = d \mid \sum w_j U_j)$ where all due dates are equal. 
Indeed, we can think of the due date $d$ as the weight of the knapsack, 
the processing times $p_j$ as the weight of the individual items, 
and the weights $w_j$ as their values. We then want to minimize the value 
of the items we are throwing out of the knapsack, which is the same 
as maximizing the value of the items inside the knapsack. 

In this section, our goal is to come up with some dynamic programming 
approaches to solve $(1\;\|\;\sum w_j U_j)$ in pseudo-polynomial time. 
We then construct an FPTAS using one of the dynamic programming schemes. 

First, we will introduce some notation. Consider an instance of 
$(1\;\|\;\sum w_j U_j)$ where all weights $w_j \geq 0$ are integer. 
We assume that the jobs are indexed by the EDD sequence. 
\begin{itemize}
    \item We call $S$ a {\bf feasible} set of jobs if every job in $S$ 
    can be completed on time starting at time $0$. 
    \item We define $w(S) = \sum_{j\in S} w_j$ and $p(S) = \sum_{j\in S} p_j$. 
\end{itemize}

{\bf Approach 1.} We define $T(i, \hat w)$ to be the minimum completion 
time of a feasible subset $S$ of $[i] = \{1, \dots, i\}$ with weight 
$\geq \hat w$. We can think of $\hat w$ as a ``target'' weight. 
In other words, $T(i, \hat w)$ is the minimum completion time of a 
chosen subset of the jobs $\{1, \dots, i\}$ such that all jobs in the 
chosen subset are timely (that is, there are no late jobs in an EDD ordering 
of the chosen subset) and moreover, the total weight of the jobs in the 
chosen subset is $\geq \hat w$.

We set $T(i, \hat w) = \infty$ if no such feasible set exists, 
$T(0, 0) = 0$, and $T(0, \hat w) = \infty$ for all $\hat w > 0$. Then 
the recurrence relation is given by 
\[ T(i, \hat w) = \begin{cases}
    \min\{T(i-1, \hat w), p_i + T(i-1, \hat w - w_i)\}, & \text{if } 
    p_i + T(i-1, \hat w - w_i) \leq d_i, \\ 
    T(i-1, \hat w), & \text{otherwise.} 
\end{cases} \] 
In the computed table for $T(i, \hat w)$, there are $n+1$ rows corresponding 
to $i = 0, \dots, n$ where $n$ is the number of jobs, and $\sum w_j + 1$ 
corresponding to the potential total weights $\hat w = 0, \dots, \sum w_j$. 
From the table, we can then find the optimal solution to the 
$(1\;\|\;\sum w_j U_j)$ instance in $O(\sum w_j)$ time.

We can actually do slightly better. If $w^*$ is the maximum value of 
$w$ such that $T(i, w^*)$ is finite, then we can compute the table for 
$T(i, \hat w)$ in $O(nw^*)$ time and find the optimal solution from the 
table in $O(w^*)$ time. So the dynamic programming algorithm runs in time 
$O(nw^*)$. 

{\bf Approach 2.} This is the approach we will use for our FPTAS. We define 
$Q(i, \hat w)$ to be the minimum completion time of a feasible subset $S$ 
of $[i]$ such that $w([i] \setminus S) \leq \hat w$. In other words, 
$Q(i, \hat w)$ is the minimum completion time of a chosen subset of the jobs 
$\{1, \dots, i\}$ such that all of the jobs in the chosen subset are timely 
(that is, there are no late jobs in an EDD ordering of the chosen subset)
and moreover, the sum of the weights of the remaining jobs in 
$\{1, \dots, i\}$ is $\leq \hat w$. 

We define $Q(0, \hat w) = 0$ for all $\hat w \geq 0$ and $Q(0, \hat w) 
= \infty$ for all $\hat w < 0$. Then the recurrence relation is given by 
\[ Q(i, \hat w) = \begin{cases}
    \min\{Q(i-1, \hat w - w_i), p_i + Q(i-1, \hat w)\}, & \text{if } 
    p_i + Q(i-1, \hat w) \leq d_i, \\ 
    Q(i-1, \hat w - w_i), & \text{otherwise.}
\end{cases} \]
In essence, this is the complement to the first approach. But why do we 
prefer this one instead? This is because it is slightly more efficient. 
We do not need to compute all of the entries of the table like in the 
first approach; we only need to start at $\hat w = 0$ and stop when 
we reach the minimum value $w^{**}$ such that $Q(i, w^{**})$ is finite. 
Observe that this minimum value $w^{**}$ is the same as the optimal value 
to the $(1\;\|\;\sum w_j U_j)$ instance. In many cases, the value of 
$w^{**}$ is much smaller than that of $w^*$ above. 

Our FPTAS then uses an extension of the above observation. If we can compute a 
lower bound $\text{LB}$ on $\text{OPT}$ such that $\text{LB} \leq 
\text{OPT} \leq n \cdot \text{LB}$, then we could replace $w^{**}$ above 
by $n \cdot \text{LB}$. We design the FPTAS by scaling and rounding the 
weights, and then applying our second dynamic programming approach above
with $Q(i, \hat w)$. This is quite similar to the FPTAS of the {\sc Knapsack} 
problem. 

There are two key building blocks for the FPTAS for $(1\;\|\;\sum w_j U_j)$. 
\begin{enumerate}[(1)]
    \item We need a lower bound $\text{LB}$ on $\text{OPT}$ computable in 
    polynomial time such that $\text{LB} \leq \text{OPT} \leq n \cdot \text{LB}$.
    \item We need an algorithm (based on dynamic programming) which computes 
    the optimal value and runs in time  polynomial in $n$ and $\text{LB}$. 
\end{enumerate}
We address (1) by solving the problem $(1\;\|\;\max_j w_jU_j)$ on the 
same instance. This can be done by applying the LCL algorithm 
(Algorithm~\ref{algo:2.10}) to the functions 
\[ h_j(C_j) = \begin{cases} 
    0, & \text{if } C_j \leq d_j, \\ 
    w_j, & \text{if } C_j > d_j. 
\end{cases} \] 
We also have the following result, and we leave its proof as an exercise. 

\begin{prop}{prop:4.15}
    For any schedule $S$, we have 
    \[ \max_j w_j U_j^S \leq \sum_j w_j U_j^S \leq n \cdot \max_j w_j U_j^S. \] 
    Moreover, if $S^*$ is an optimal schedule for $(1\;\|\;\max_j w_jU_j)$, then 
    \[ \max_j w_j U_j^{S^*} \leq \text{OPT} \leq n \cdot \max_j w_j U_j^{S^*}, \] 
    where $\text{OPT}$ is the optimal value for the $(1\;\|\;\sum w_j U_j)$ instance. 
\end{prop}

We set $\text{LB}$ to be the optimal value for the $(1\;\|\;\max_j w_j U_j)$ 
instance. Our scale and round step is straightforward. Given an error 
parameter $\eps > 0$, let the scaling parameter be $\delta = \eps \cdot 
\text{LB}/n$. We define $w'_j = \lfloor w_j/\delta \rfloor$ for all jobs $j$. 
It is easy to see that $w_j/\delta - 1 \leq w'_j \leq w_j/\delta$. 

We apply the second dynamic programming algorithm to the rounded instance 
with the weights $w'$. The running time is polynomial in $n$ and 
$\text{LB}'$ where $\text{LB}' = \text{LB}/\delta = n/\eps$ because 
the optimal value of the rounded instance is $\leq n \cdot \text{LB}'$.

For the rounded instance, let $\text{OPT}'$ denote the optimal value. 
Let $A'$ be the set of late jobs in an optimal schedule, so $w'(A') = 
\text{OPT}'$. Let $A^*$ be the set of late jobs in an optimal schedule for 
the original instance so that $w(A^*) = \text{OPT}$. 

We claim that $w(A') \leq (1+\eps) \text{OPT}$. This follows from applying some
inequalities we have stated earlier and the fact that $w'(A') \leq w'(A^*)$ 
since $A'$ is optimal for the weights $w'$. Indeed, we have 
\begin{align*} 
    w(A') &= \sum_{j\in A'} w_j \leq \sum_{j\in A'} (\delta w'_j + \delta) \\
    &\leq \delta w'(A') + n\delta \\ 
    &\leq \delta w'(A^*) + n\delta \\
    &\leq w(A^*) + \eps \cdot \text{LB} \\ 
    &\leq (1 + \eps) w(A^*) \\ 
    &= (1 + \eps) \text{OPT}. 
\end{align*}
Thus, we see that our algorithm is an FPTAS. 
