\section{More Single Machine Models} \label{sec:4}

\subsection{Maximum Lateness with Release Dates} \label{subsec:4.1}
We have already seen in Section~\ref{subsec:2.3} that there is a polynomial 
time algorithm to solve $(1\;\|\;L_{\max})$ instances. This was the 
Earliest Due Date (EDD) rule, which placed the jobs in increasing order of the 
due dates. We also saw that this was a special case of $(1 \mid \text{prec} 
\mid h_{\max})$, for which there was also an efficient algorithm, namely 
Lowest Cost Last (LCL). 

But what if we introduce release dates to the $(1\;\|\;L_{\max})$ problem? 
It turns out that this generalization, without preemption, is significantly 
harder than the problem where all jobs are available at time $0$. Moreover, 
the optimal schedule is not necessarily a non-delay schedule. It can be 
advantageous in this case to keep the machine idle before the release of a 
new job. 

\begin{theo}{theo:4.1}
    The problem $(1 \mid r_j \mid L_{\max})$ is strongly $\NP$-hard. 
\end{theo}
\begin{pf}
    This proof is based on the fact that {\sc $3$-Partition} (as described 
    in Example~\ref{exmp:3.14}) reduces to $(1 \mid r_j \mid L_{\max})$. 
    Suppose that we are given integers $a_1, \dots, a_{3t}, b$ such that 
    $\frac{b}{4} < a_j < \frac{b}{2}$ and $\sum_{j=1}^{3t} a_j = t \cdot b$. 
    We construct an instance of $(1 \mid r_j \mid L_{\max})$ with 
    $n = 4t - 1$ as follows: 
    \begin{itemize}
        \item For $j = 1, \dots, t-1$, we set $r_j = jb + (j-1)$, $p_j = 1$, 
        $d_j = jb + j$. 
        \item For $j = t, \dots, 4t-1$, we set $r_j = 0$, $p_j = a_{j-t+1}$, 
        and $d_j = tb + (t-1)$. 
    \end{itemize}
    Notice that a schedule with $L_{\max} \leq 0$ exists if and only if 
    every job $j \in \{1, \dots, t-1\}$ can be processed between 
    $r_j$ and $d_j = r_j + p_j$. This can be done if and only if the remaining 
    jobs can be partitioned over the $t$ intervals of length $b$, which can be 
    done if and only if {\sc $3$-Partition} has a solution. 
\end{pf}

The $(1 \mid r_j \mid L_{\max})$ problem is important because it often 
appears as a subproblem in heuristic procedures for flow shop and job shop 
problems. A branch and bound procedure $(1 \mid r_j \mid L_{\max})$ 
can be constructed as follows. The branching process may be based on the 
fact that schedules are developed starting from the beginning of the schedule. 
There is a single node at level $0$ which is the top of the tree. At this 
node, no job has been put into any position of the sequence yet. There are 
$n$ branches going down to $n$ nodes at level $1$. Each node at this level 
has a specific job put into the first position of the schedule. Then
at each node, there are $n-1$ jobs remaining whose position in the schedule 
has yet to be determined. Hence, there are $n-1$ arcs emanating from each 
node at level $1$ to level $2$, and there are $(n-1) \times (n-2)$ nodes at 
level $2$. We could continue in this way to enumerate all possible schedules. 

However, it is not necessary to consider every remaining job as a possible 
candidate for the next position. If the jobs $j_1, \dots, j_{k-1}$ 
are scheduled as the first $k-1$ jobs at a node at level $k-1$, then 
we only need to consider job $j_k$ if 
\[ r_{j_k} < \min_{\ell \in J} \left( \max(t, r_\ell) + p_\ell \right), \] 
where $J$ denotes the set of jobs not yet scheduled and $t$ denotes the 
time job $j_k$ is supposed to start. The reason for this condition is because 
if job $j_k$ does not satisfy this inequality, then selecting the job 
which minimizes the right-hand side instead of $j_k$ does not increase the 
value of $L_{\max}$. Therefore, the branching rule is fairly easy. 

There are several ways in which bounds for nodes can be obtained. 
One easy lower bound for a node at level $k-1$ can be established by 
scheduling the remaining jobs $J$ according to the \emph{preemptive} 
EDD rule which is known to be optimal for $(1 \mid r_j, \text{prmp} \mid 
L_{\max})$, and thus provides a lower bound for the problem at hand. 
If a preemptive EDD rule results in a non-preemptive schedule, then 
all nodes with a higher lower bound may be disregarded. 

\begin{exmp}{exmp:4.2}
    Consider the following instance of $(1 \mid r_j \mid L_{\max})$ with 
    $n = 4$ jobs. 
    \begin{align*}
        \begin{array}{c|cccc} 
            j & 1 & 2 & 3 & 4 \\ \hline 
            p_j & 4 & 2 & 6 & 5 \\ 
            r_j & 0 & 1 & 3 & 5 \\ 
            d_j & 8 & 12 & 11 & 10 
        \end{array}
    \end{align*}
    At level $1$ of the search tree, there are four nodes, namely 
    $(1, *, *, *)$, $(2, *, *, *)$, $(3, *, *, *)$, and $(4, *, *, *)$. 
    Notice that we may discard the nodes $(3, *, *, *)$ and 
    $(4, *, *, *)$ immediately. Indeed, we have $\min_{\ell\in \{1, 2, 3, 4\}} 
    (\max(t, r_\ell) + p_\ell) = 3$ with $\ell = 2$, and we see that 
    $r_3 \geq 3$ and $r_4 \geq 3$. 

    Computing a lower bound for node $(1, *, *, *)$ according to the 
    preemptive EDD rule results in a schedule where job $3$ is processed 
    during the interval $[4, 5]$, job $4$ is processed during $[5, 10]$, 
    job $3$ again during $[10, 15]$, and job $2$ during $[15, 17]$. 
    Then $L_{\max} = 5$ for this schedule, and so $5$ is a lower bound 
    for the node $(1, *, *, *)$. A similar computation shows that a lower 
    bound for the node $(2, *, *, *)$ is $7$. 

    Consider now the node $(1, 2, *, *)$ at level $2$. The lower bound 
    for this node is $6$ and is determined by the non-preemptive 
    schedule $1, 2, 4, 3$. Next, looking at the node $(1, 3, *, *)$ 
    at level $2$, the lower bound is $5$ and is determined by the 
    non-preemptive schedule $1, 3, 4, 2$. Since the lower bound for node 
    $(1, *, *, *)$ is $5$ and the lower bound for $(2, *, *, *)$ is 
    greater than $5$, it follows that the schedule $1, 3, 4, 2$ is optimal. 
\end{exmp}

The problem $(1 \mid r_j, \text{prec} \mid L_{\max})$ can be handled in a similar 
way. From an enumeration point of view, it is even easier than the problem 
without precedence constraints since many schedules can be ruled out 
immediately. 

\subsection{The Number of Tardy Jobs} \label{subsec:4.2}
Recall that $U_j = 0$ if the job is timely and $U_j = 1$ if the job is late. 
The goal of the problem $(1\;\|\;\sum U_j)$ is to minimize the number of 
tardy jobs. This objective may at first appear somewhat artificial and 
seems to be of no practical interest. However, in the real world, it is a 
performance measure that is often monitored. It is equivalent to the 
percentage of on time shipments.

Notice that it does not matter how late a job is; the only 
determining factor is if it is late or not. A solution to this problem can 
be represented as a partition of the jobs into sets $S_1$ and $S_2$, where 
$S_1$ is the set of jobs meeting their due dates in Earliest Due Date (EDD)
order, and $S_2$ is the set of late jobs in an arbitrary order (since the 
amount of lateness is irrelevant).  

\begin{lemma}{lemma:4.3}
    Let OPT denote the optimal value for a given instance of $(1\;\|\;\sum U_j)$.
    If the sequence given by the EDD rule has a late job, then $\text{OPT} \geq 1$. 
\end{lemma}
\begin{pf}
    Let $k$ be the first late job in the EDD sequence. Then we have 
    \[ C_k = \sum_{i\in[k]} p_i > d_k = \max_{i\in[k]} d_i. \] 
    Consider any schedule $S$. Let $\ell$ be the last of the jobs in $[k]$ in 
    $S$. Then the completion time of $\ell$ in $S$ is at least $\sum_{i\in[k]} 
    p_i > d_\ell$, so $\ell$ is a late job in $S$. 
\end{pf}

\begin{algo}[Moore-Hodgson]{algo:4.4}
    \begin{enumerate}
        \item Enumerate the jobs in EDD order. 
        \item Set $S_1 \gets \varnothing$ and $t \gets 0$. 
        \item for $j=1$ to $n$ do: 
        \begin{itemize}[\label{}]
            \item Set $S_1 \gets S_1 \cup \{j\}$ and $t \gets t + p_j$. 
            \item if $t > d_j$ then: 
            \begin{itemize}[\label{}]
                \item Find a job $k$ with the largest $p_k$ value in $S_1$. 
                \item Set $S_1 \gets S_1 \setminus \{k\}$ and $t \gets t - p_k$. 
            \end{itemize}
            endif
        \end{itemize}
        endfor
    \end{enumerate}
\end{algo}

The principle of the Moore-Hodgson algorithm is that we schedule the jobs by the EDD 
rule, and when a job gets late, we rescue the situation by throwing out the job with 
the highest processing time. All removed jobs are considered late, and the remaining 
ones are timely. This algorithm runs in $O(n\log n)$ time. 

\begin{exmp}{exmp:4.5}
    Consider the following instance of $(1 \;\|\; \sum U_j)$ with $n = 5$ jobs, 
    where the jobs are already in EDD order.  
    \begin{align*}
        \begin{array}{c|ccccc}
            j   & 1 & 2  & 3  & 4  & 5 \\ \hline 
            p_j & 7 & 8  & 4  & 6  & 6 \\ 
            d_j & 9 & 17 & 18 & 19 & 21
        \end{array}
    \end{align*}
    We can initially schedule jobs $1$ and $2$, which will both be timely. However, 
    once we schedule job $3$, we see that it will be late, since $t = 19 > 18 = d_3$.
    \begin{verbatim}
        |---1---|---2----|-3--|
        0       7        15   19
    \end{verbatim}
    \vspace{-1em}
    Therefore, we toss out job $2$ which has the highest processing time of $p_2 = 8$
    and continue. We can schedule job $4$ just fine, but job $5$ will be late with 
    $t = 23 > 21 = d_5$. 
    \begin{verbatim}
        |---1---|-3--|--4---|--5---|
        0       7    11     17     23
    \end{verbatim}
    \vspace{-1em}
    This time, we toss out job $1$ since it has the highest processing time of $p_1 = 7$. 
    Then we obtain $S_1 = \{3, 4, 5\}$ and $S_2 = \{1, 2\}$, so $\text{OPT} = 2$ for 
    this instance. 
\end{exmp}

The following lemma is the key to proving that the Moore-Hodgson algorithm is 
optimal for $(1\;\|\;\sum U_j)$. We will assume that the jobs are already scheduled 
in EDD. 

\begin{lemma}{lemma:4.6}
    Suppose there is at least one late job in the EDD sequence $1, \dots, n$. 
    Let $k$ be the first late job in the EDD sequence, and let $m$ be the first 
    job rejected by the Moore-Hodgson algorithm. Then there is an optimal 
    schedule which rejects $m$. 
\end{lemma}
\begin{pf}
    Consider any optimal schedule $\pi$. Let $R_\pi \subseteq [n]$ denote 
    the subset of rejected (late) jobs, and let $A_\pi = [n] \setminus R_\pi$ 
    denote the set of timely jobs. By Lemma~\ref{lemma:4.3}, we may assume that 
    $\pi$ schedules the jobs of $A_\pi$ in EDD order, followed by the 
    jobs of $R_\pi$ in arbitrary order. 

    If $m \in R_\pi$, we are done. So suppose that $m \notin R_\pi$. 
    By Lemma~\ref{lemma:4.3}, there is a job $r \in [k]$ other than $m$ 
    that has been rejected. Consider the schedule $\sigma$ that sequences 
    the jobs in $A_\sigma = (A_\pi \setminus \{m\}) \cup \{r\}$ in EDD 
    order first, followed by the jobs in $[n] \setminus A_\sigma = 
    (R_\pi \setminus \{r\}) \cup \{m\}$ in arbitrary order. We will show that 
    $\sigma$ schedules all jobs in $A_\sigma$ on time. This will mean that 
    $\sigma$ is an optimal schedule since $|R_\sigma| = |R_\pi|$, 
    and by construction, $\sigma$ rejects $m$.

    First, the jobs in $A_\sigma \cap [k-1]$ are completed on time 
    since the EDD rule completes all jobs in $[k-1]$ on time. Next, 
    if $k \in A_\sigma$, then its completion time is at most 
    \[ \sum_{i\in[k] \setminus \{m\}} p_i \leq 
    \sum_{i\in[k-1]} p_i \leq d_{k-1} \leq d_k, \] 
    where the first inequality follows from the choice of $m$ by the 
    Moore-Hodgson algorithm which ensures that $p_m = 
    \max_{i\in[k]} p_i \geq p_k$. Finally, compared to the schedule $\pi$, 
    the completion times of the jobs in $A_\sigma \setminus [k] = 
    A_\pi \setminus [k]$ have been changed in the new schedule $\sigma$
    by $p_r - p_m \leq 0$, so these jobs are also on time. 
\end{pf}
 
\begin{theo}{theo:4.7}
    The Moore-Hodgson algorithm gives an optimal schedule for 
    $(1\;\|\;\sum U_j)$. 
\end{theo}
\begin{pf}
    We proceed by induction on $\text{OPT}$ over all instances of the problem. 

    If an instance has $\text{OPT} = 0$, then by Lemma~\ref{lemma:4.3}, the 
    Moore-Hodgson algorithm outputs the EDD sequence with no late jobs. 

    Suppose now that $I$ is an instance with $\text{OPT}(I) \geq 1$ late 
    jobs. Let $I'$ be obtained from $I$ by deleting the first job $m$ 
    which is rejected by the Moore-Hodgson algorithm. By Lemma~\ref{lemma:4.6}, 
    we have $\text{OPT}(I') = \text{OPT}(I) - 1$. By the induction hypothesis, 
    the algorithm finds an optimal schedule $S'$ for $I'$. The algorithm 
    for $I$ outputs the schedule $S$ such that $S$ is the same as $S'$ 
    except that job $m$ is added at the end as a rejected job. Clearly, 
    $S$ has at most $\text{OPT}(I') + 1 = \text{OPT}(I)$ rejected jobs 
    and hence is optimal. 
\end{pf}

\subsection{Dynamic Programming for Total Tardiness} \label{subsec:4.3}
In practice, minimizing the number of tardy jobs $\sum U_j$ cannot be the 
only objective to measure how due dates are being met. Only minimizing 
the number of late jobs will force some jobs to have to wait for an 
unacceptably long time to complete. If we instead minimize the total 
tardiness, it is less likely that the wait for any given job will be 
unacceptably long. 

The problem $(1\;\|\;\sum T_j)$ has received an enormous amount of attention 
in literature. For many years, its computational complexity remained open; 
its $\NP$-hardness was only established recently in 1990. Since 
$(1\;\|\;\sum T_j)$ is $\NP$-hard in the ordinary sense, it allows for a 
pseudo-polynomial time algorithm based on dynamic programming. The 
algorithm is based on two preliminary results. 

\begin{lemma}{lemma:4.8}
    If $p_j \leq p_k$ and $d_j \leq d_k$, then there exists an optimal schedule 
    in which job $j$ is scheduled before job $k$. 
\end{lemma}
\begin{pf}
    We leave the proof as an exercise. This is a standard interchange 
    argument. 
\end{pf}

This type of result is useful when an algorithm has to be developed for
a problem that is $\NP$-hard. Such a result, often referred to as a 
{\bf dominance result} or {\bf elimination criterion}, often allows one to
disregard a fairly large number of sequences. Such a dominance result may also 
be thought of as a set of precedence constraints on the jobs. The more 
precedence constraints created through such dominance results, the easier the 
problem becomes.

In the following lemma, the sensitivity of an optimal sequence to the due
dates is considered. We consider two problem instances, both of which have 
$n$ jobs with processing times $p_1, \dots, p_n$. The first instance has 
due dates $d_1, \dots, d_n$. Let $k$ be a fixed job, and let $\hat C_k$ 
be the latest completion time of job $k$ among all optimal schedules. 
Then we set the due dates for the second instance to be 
\[ \hat d_j = \begin{cases}
    d_j, & \text{if } j \neq k, \\ 
    \max(d_k, \hat C_k), & \text{if } j = k.
\end{cases} \] 
In particular, we are only changing one piece of data, namely the 
due date of job $k$. 

\begin{lemma}{lemma:4.9}
    Any optimal sequence with respect to the second instance with due dates 
    $\hat d_1, \dots, \hat d_n$ is also optimal for the first instance 
    with due dates $d_1, \dots, d_n$. 
\end{lemma}
\begin{pf}
    We first introduce some notation. Let $\hat S$ be an optimal sequence 
    such that job $k$ has largest completion time $\hat C_k$ among 
    all optimal sequences. For an arbitrary sequence $S$, we let 
    $V(S)$ be the objective value according to the first instance 
    where job $k$ has due date $d_k$, and $V'(S)$ be the objective 
    value according to the second instance where job $k$ has due date 
    $\hat d_k$. 

    Let $S'$ be any optimal sequence for the second instance. Then we have 
    $V'(\hat S) \geq V'(S')$ since $S'$ is optimal for the second instance. 
    Moreover, we observe that 
    \[ V'(\hat S) = V(\hat S) - (\hat C_k - d_k), \] 
    or equivalently, $V(\hat S) - V'(\hat S) = \hat C_k - d_k$. Finally, 
    when we switch objectives, the most improvement we can get is 
    $\hat C_k - d_k$, so we deduce that 
    \begin{align*}
        V(S') &\leq V'(S') + (\hat C_k - d_k) \\ 
        &= V'(S') + V(\hat S) - V'(\hat S) \\ 
        &\leq V(\hat S).
    \end{align*}
    This means that $S'$ is also optimal for the first instance, so we are done. 
\end{pf}

Lemma~\ref{lemma:4.9} is quite a technical result, and it requires the 
knowledge of all optimal sequences for the first instance. 
For now, we will assume that the jobs are scheduled in EDD order 
so that $d_1 \leq \cdots \leq d_n$, and job $k$ is such that 
$p_k = \max(p_1, \dots, p_n)$. In particular, this is the job with 
the $k$-th smallest due date and largest processing time. It follows from 
Lemma~\ref{lemma:4.8} that there exists an optimal sequence in which 
jobs $1, \dots, k-1$ all appear in some order before job $k$. Of the remaining 
$n-k$ jobs, some may appear before job $k$ while some may appear after job $k$. 
The following lemma focuses on the placement of these remaining jobs. 

\begin{lemma}{lemma:4.10}
    There is an integer $\delta$ with $0 \leq \delta \leq n-k$ such that there 
    is an optimal sequence $S$ in which job $k$ is preceded by all jobs $j$ 
    with $j \leq k + \delta$ and followed by all jobs $j$ with $j > k + \delta$. 
\end{lemma}
\begin{pf}
    We only give a brief sketch here. In some optimal schedule, the jobs 
    $1, \dots, k-1$ are positioned before 
    job $k$ due to the dominance criterion from Lemma~\ref{lemma:4.8}. 
    From Lemma~\ref{lemma:4.9}, we can increase $d_k$ such that some other 
    jobs $k+1, \dots, k+\delta$ are positioned before job $k$ due to the 
    dominance criterion. 
\end{pf}

Lemma~\ref{lemma:4.10} tells us an optimal sequence looks like the concatenation 
of three subsets of jobs, namely 
\begin{enumerate}[(i)]
    \item jobs $1, \dots, k-1, k+1, \dots, k+\delta$ in some order, followed by 
    \item job $k$, followed by 
    \item jobs $k+\delta+1, \dots, n$ in some order. 
\end{enumerate}
The completion time of job $k$ under this schedule is 
\[ C_k(\delta) = \sum_{j\leq k+\delta} p_j. \] 
For the entire sequence to be optimal, it is clear that the first and third 
subsets must be optimally sequenced within themselves. This is the heart of 
a dynamic programming approach that determines an optimal sequence for a 
larger set of jobs after having determined optimal sequences for proper 
subsets of the larger set. 

We define $J(j, \ell, k)$ to be the set of all jobs in the set $\{j, \dots, \ell\}$ 
with processing time at most $p_k$, but $k \notin J(j, \ell, k)$. 
Note that here, $k$ does not necessarily have to be the job with highest processing 
time. Then, we define $V(J(j, \ell, k), t)$ to be the total tardiness of 
the jobs $J(j, \ell, k)$ in an optimal sequence that starts at time $t$. 
We now state the dynamic programming algorithm. 

\begin{algo}[Dynamic Programming for Total Tardiness]{algo:4.11}
    The initial conditions are $V(\varnothing, t) = 0$ and 
    $V(\{j\}, t) = \max(0, t + p_j - d_j)$ for all jobs $j$ and $t \geq 0$. 
    We have the recursive relation 
    \[ V(J(j, \ell, k), t) = \min_{\delta} \{V(J(j, k'+\delta, k'), t)
    + \max(0, C_{k'}(\delta) - d_{k'}) + V(J(k'+\delta+1, \ell, k'), 
    C_{k'}(\delta))\}, \] 
    where job $k'$ is such that $p_{k'} = \max_{j'\in J(j,\ell, k)} p_{j'}$. 
    The optimal value is then given by $V(\{1, \dots, n\}, 0)$. 
\end{algo}

Note that there are at most $O(n^3)$ subsets of jobs $J(j, \ell, k)$ and 
$\sum p_j$ possible values of $t$. This means that there are 
$O(n^3 \sum p_j)$ recursive equations. Each recursion takes $O(n)$ time, 
so the total running time of the dynamic programming approach is 
$O(n^4 \sum p_j)$, which is pseudo-polynomial. 

\begin{exmp}{exmp:4.12}
    We run our dynamic programming algorithm on an example. Consider 
    the following instance of $(1\;\|\;\sum T_j)$ with $n = 5$ jobs. 
    \begin{align*}
        \begin{array}{c|ccccc}
            j & 1 & 2 & 3 & 4 & 5 \\ \hline 
            p_j & 121 & 79 & 147 & 83 & 130 \\ 
            d_j & 260 & 266 & 266 & 336 & 337 
        \end{array}
    \end{align*}
    We see that job $3$ has the largest processing time, giving us 
    $0 \leq \delta \leq 5 - 3 = 2$. Then we have 
    \[ V(\{1, 2, 3, 4, 5\}, 0) = \min\begin{cases} 
        V(J(1, 3, 3), 0) + 81 + V(J(4, 5, 3), 347), & \text{for } \delta = 0, \\ 
        V(J(1, 4, 3), 0) + 164 + V(J(5, 5, 3), 430), & \text{for } \delta = 1, \\ 
        V(J(1, 5, 3), 0) + 294 + V(\varnothing, 560), & \text{for } \delta = 2. 
    \end{cases} \] 
    We see that $V(J(1, 3, 3), 0) = 0$ via the sequences $1, 2$ and $2, 1$. 
    The dominance rule tells us that the sequences $1, 2, 4$ and $2, 1, 4$ 
    are optimal for the jobs $J(1, 4, 3)$ with $V(J(1, 4, 3), 0) = 0$, 
    and similarly, the sequences $1, 2, 4, 5$ and $2, 1, 4, 5$ are optimal for 
    $J(1, 5, 3)$ with $V(J(1, 5, 3), 0) = 76$.

    On the other hand, we have 
    \[ V(J(4, 5, 3), 347) = (347 + 83 - 336) + (347 + 83 + 130 - 337) = 317 \] 
    for the sequence $4, 5$, and $V(J(5, 5, 3), 430) = 430 + 130 - 337 = 223$.

    Putting everything together, we obtain 
    \[ V(\{1, 2, 3, 4, 5\}, 0) = 
    \min\{0 + 81 + 317, 0 + 164 + 223, 76 + 294 + 0\}
    = \min\{398, 387, 370\} = 370. \] 
    The sequences that attain this value are $1, 2, 4, 5, 3$ and $2, 1, 4, 5, 3$.    
\end{exmp}

\subsection{FPTAS for Total Tardiness} \label{subsec:4.3}
Since $(1\;\|\;\sum T_j)$ is $\NP$-hard, we cannot hope to find a polynomial 
time algorithm to solve arbitrary instances of it. However, we can give an FPTAS
to obtain a solution that is close to optimal, using a similar approach 
as we did for the {\sc Knapsack} problem in Section~\ref{subsec:3.4}. 

First, we will give some lower and upper bounds for total tardiness. 
\begin{itemize}
    \item Recall that the EDD rule produces a schedule with optimal 
    maximum lateness. In particular, this also produces a schedule 
    with optimal maximum tardiness. 
    \item Clearly, at least one job in a schedule has the maximum tardiness. 
    Then the total tardiness of an optimal schedule is at least the 
    optimal maximum tardiness. 
    \item The total tardiness of the EDD schedule is at least the optimal 
    maximum tardiness, but at most $n$ times the optimal maximum tardiness. 
\end{itemize}
Let $\sum T_j(\text{EDD})$ denote the total tardiness under the EDD schedule, 
and let $T_{\max}(\text{EDD}) = \max(T_1, \dots, T_n)$ be the maximum 
tardiness under the EDD schedule. For an optimal schedule $\text{OPT}$, we have 
\[ T_{\max}(\text{EDD}) \leq \sum T_j(\text{OPT}) \leq \sum T_j(\text{EDD}) 
\leq n \cdot T_{\max}(\text{EDD}). \] 
Let $V(J, t)$ be the minimum total tardiness of the job subset $J$ 
assuming that processing begins at time $t$. There is a time $t^*$ 
such that $V(J, t) = 0$ for all $t \leq t^*$ and $V(J, t) > 0$ for all 
$t > t^*$. Then we have $V(J, t^* + \delta) \geq \delta$ for all 
$\delta \geq 0$. By executing the pseudo-polynomial dynamic programming 
approach described in Algorithm~\ref{algo:4.11}, one only has to compute 
$V(J, t)$ for 
\[ \max\{0, t^*\} \leq t \leq t^* + n \cdot T_{\max}(\text{EDD}). \] 
Substituting $\sum p_j$ in the overall running time of the dynamic 
programming algorithm by $n \cdot T_{\max}(\text{EDD})$ yields a 
new bound of $O(n^5 \cdot T_{\max}(\text{EDD}))$. 

Now, rescale the processing times and due dates via $p'_j = 
\lfloor p_j/K \rfloor$ and $d'_j = d_j/K$ for some factor $K > 0$. 
Let $S$ be an optimal schedule for the rescaled problem. We let 
$\sum T_j^*(S)$ denote the total tardiness of the schedule $S$ with 
respect to the processing times $Kp'_j \leq p_j$ and due dates $d_j$. 
Let $\sum T_j(S)$ denote the total tardiness of the schedule $S$ with 
respect to the original processing times $p_j$ and due dates $d_j$. 
From the fact that $Kp'_j \leq p_j < K(p'_j + 1)$, it follows that 
\[ \sum T_j^*(S) \leq \sum T_j(\text{OPT}) \leq \sum T_j(S) < 
\sum T_j^*(S) + \frac{Kn(n+1)}{2}. \] 
This chain of inequalities implies that 
\[ \sum T_j(S) - \sum T_j(\text{OPT}) < \frac{Kn(n+1)}{2}. \] 
If $K$ is chosen such that 
\[ K = \frac{2\eps}{n(n+1)} \cdot T_{\max}(\text{EDD}), \] 
then we obtain 
\[ \sum T_j(S) - \sum T_j(\text{OPT}) < \eps \cdot T_{\max}(\text{EDD}). \] 
Moreover, for this choice of $K$, the time bound 
$O(n^5 \cdot T_{\max}(\text{EDD})/K)$ becomes $O(n^7/\eps)$, making 
our approximation scheme fully polynomial. We summarize the FPTAS 
as follows. 

\begin{algo}[Total Tardiness FPTAS]{algo:4.13}
    \begin{enumerate}[(1)]
        \item Apply EDD in order to determine $T_{\max}(\text{EDD})$. 
        If $T_{\max}(\text{EDD}) = 0$, then $\sum T_j(\text{OPT}) = 0$ 
        and EDD gives an optimal schedule, so stop. Otherwise, set 
        \[ K = \frac{2\eps}{n(n+1)} \cdot T_{\max}(\text{EDD}). \] 
        \item Rescale the processing times and due dates via 
        $p'_j = \lfloor p_j/K \rfloor$ and $d'_j = d_j/K$. 
        \item Apply Algorithm~\ref{algo:4.11} to the rescaled data. 
    \end{enumerate}
\end{algo}